{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook needs to be converted to python file and run with deepspeed filename.py\n",
    "### DeepSpeed needs to have https://github.com/microsoft/DeepSpeed/pull/5780 integrated into deepspeed/ops/op_builder/builder.py\n",
    "### LongformerSelfAttention forward have a line that needs to be changed\n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, :, None, None]\n",
    "### have incorrect shape because the thing that is passed in is [a, 1, 1, b]\n",
    "### so need to change it to \n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, 0, 0, :, None, None]\n",
    "### Another problem occurs where the code in the same function\n",
    "### attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n",
    "### does not check if is_index_masked is None\n",
    "### So this needs to be changed to\n",
    "### if is_index_masked:\n",
    "###     attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import BertForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser, AutoModelForMaskedLM, BertTokenizerFast\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_save_path = \"tmp_save\"\n",
    "if not os.path.exists(tmp_save_path):\n",
    "    os.mkdir(tmp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalBertLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class LegalBertLongForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.bert.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = LegalBertLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "model_args = ModelArgs()\n",
    "\n",
    "model_hidden_size = 512\n",
    "train_batch_size = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"tmp\",\n",
    "    max_steps = 3000,\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    deepspeed=\"ds_config.json\",\n",
    ")\n",
    "training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
    "training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
    "\n",
    "model_path = f'{training_args.output_dir}/legalbert-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "legalbert_tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "legalbert_model = LegalBertLongForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the contractnli dataset\n",
    "\n",
    "### load contractnli\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json\n",
    "from transformers import PerceiverTokenizer, PerceiverModel, PerceiverConfig, PerceiverPreTrainedModel, PerceiverForSequenceClassification, TrainingArguments, Trainer, \\\n",
    "    DataCollatorWithPadding, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "ROOT_PATH = \"/home/yan_xu_uk_qbe_com/scc_yan/\"\n",
    "\n",
    "with open(os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli/train.json\")) as train_json_f:\n",
    "    train_json = json.load(train_json_f)\n",
    "\n",
    "id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMnetioned\"}\n",
    "label2id = {\"Entailment\": 0, \"Contradiction\": 1, \"NotMentioned\": 2}\n",
    "\n",
    "def load_dataset_custom(dataset_name):\n",
    "    if dataset_name == \"contract-nli\":\n",
    "        def contract_nli_iterator(data):\n",
    "            documents, labels = data['documents'], data['labels']\n",
    "            for document in documents:\n",
    "                id = document['id']\n",
    "                file_name = document['file_name']\n",
    "                text = document['text']\n",
    "                spans = document['spans']\n",
    "                annotation_sets = document['annotation_sets']\n",
    "                document_type = document['document_type']\n",
    "                url = document['url']\n",
    "                for annotation_id, annotation_content in annotation_sets[0]['annotations'].items():\n",
    "                    hypothesis = labels[annotation_id]['hypothesis']\n",
    "                    choice = annotation_content['choice']\n",
    "                    yield {\n",
    "                        \"id\": id,\n",
    "                        \"file_name\": file_name,\n",
    "                        \"text\": text,\n",
    "                        \"spans\": spans,\n",
    "                        \"document_type\": document_type,\n",
    "                        \"url\": url,\n",
    "                        \"hypothesis\": hypothesis,\n",
    "                        \"labels\": label2id[choice],\n",
    "                    }            \n",
    "        base_filepath = os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli\")\n",
    "        train_filepath = os.path.join(base_filepath, \"train.json\")\n",
    "        validation_filepath = os.path.join(base_filepath, \"dev.json\")\n",
    "        test_filepath = os.path.join(base_filepath, \"test.json\")\n",
    "        with open(train_filepath) as f:\n",
    "            train_data = json.load(f)\n",
    "        with open(validation_filepath) as f:\n",
    "            validation_data = json.load(f)\n",
    "        with open(test_filepath) as f:\n",
    "            test_data = json.load(f)\n",
    "        data = {\n",
    "            \"train\": Dataset.from_generator(lambda: contract_nli_iterator(train_data)),\n",
    "            \"validation\": Dataset.from_generator(lambda: contract_nli_iterator(validation_data)),\n",
    "            \"test\": Dataset.from_generator(lambda: contract_nli_iterator(test_data)),\n",
    "        }\n",
    "        return DatasetDict(data)\n",
    "    return None\n",
    "\n",
    "contractnli_dataset = load_dataset_custom(\"contract-nli\")\n",
    "\n",
    "### chunk contractnli dataset into sizes of 512\n",
    "\n",
    "def chunk_contractnli(ds, cs, os):\n",
    "    sep_token = legalbert_tokenizer.sep_token\n",
    "    concat_text = sep_token.join([sep_token.join([e['text'], e['hypothesis']]) for e in ds])\n",
    "    tokenized_text = legalbert_tokenizer(concat_text).input_ids\n",
    "    i = 0\n",
    "    chunks = []\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while i < len(tokenized_text):\n",
    "            if i + cs >= len(tokenized_text):\n",
    "                chunks.append(tokenized_text[i:])\n",
    "                i = len(tokenized_text)\n",
    "            else:\n",
    "                chunks.append(tokenized_text[i: i + cs])\n",
    "                i += (cs - os)\n",
    "            pbar.update(i / len(tokenized_text))\n",
    "    return Dataset.from_dict({'input_ids': chunks})\n",
    "\n",
    "cs = 4096\n",
    "os = 100\n",
    "contractnli_chunked = DatasetDict({mode: chunk_contractnli(contractnli_dataset[mode], cs, os) for mode in ['train', 'validation', 'test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    val_dataset = contractnli_chunked['validation']\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        train_dataset = contractnli_chunked['train']\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=train_dataset, eval_dataset=val_dataset) # , prediction_loss_only=True,)\n",
    "\n",
    "    ### CANT DO EVALUATE HERE. IT MESSES UP WITH SETTING UP DEEPSPEED FOR TRAINING ### \n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "\n",
    "    \"\"\"  \n",
    "    if not eval_only:\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n",
    "    \"\"\"\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Pretraining roberta-base-{model_args.max_pos} ... ')\n",
    "\n",
    "training_args.max_steps = 3   ## <<<<<<<<<<<<<<<<<<<<<<<< REMOVE THIS <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "trainer = pretrain_and_evaluate(training_args, legalbert_model, legalbert_tokenizer, eval_only=False, model_path=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://deepspeed.readthedocs.io/en/latest/model-checkpointing.html\n",
    "trainer.deepspeed.save_checkpoint(tmp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n",
    "\n",
    "# state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_save_path) # already on cpu\n",
    "# print({k: type(v) for k, v in state_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n",
    "# loaded_model = LegalBertLongForMaskedLM.from_pretrained(model_path)\n",
    "# loaded_model = load_state_dict_from_zero_checkpoint(loaded_model, tmp_save_path, ignore_mismatched_sizes=True)\n",
    "# loaded_model.load_state_dict(state_dict, strict=False)\n",
    "# loaded_model.load_state_dict(state_dict)\n",
    "# print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tmp_save_path = \"tmp_save\"\n",
    "rank = torch.distributed.get_rank()\n",
    "if rank == 0:\n",
    "    if not os.path.exists(tmp_save_path):\n",
    "        os.mkdir(tmp_save_path)\n",
    "trainer.deepspeed.save_16bit_model(tmp_save_path, \"tmp_filename\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rank = torch.distributed.get_rank()\n",
    "if rank == 0:\n",
    "    tmp_save_path = \"tmp_save\"\n",
    "    if not os.path.exists(tmp_save_path):\n",
    "        os.mkdir(tmp_save_path)\n",
    "    model.save_pretrained(tmp_save_path)\n",
    "\n",
    "    logger.info(f'Loading the model from {tmp_save_path}')\n",
    "    #tokenizer = RobertaTokenizerFast.from_pretrained(tmp_save_path)\n",
    "    #print(\"tokenizer has been loaded\")\n",
    "    model = RobertaLongForMaskedLM.from_pretrained(tmp_save_path)\n",
    "    print(model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
    "model = copy_proj_layers(model)\n",
    "logger.info(f'Saving model to {model_path}')\n",
    "model.save_pretrained(model_path)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
