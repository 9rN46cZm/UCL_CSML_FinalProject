{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import BertForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser, AutoModelForMaskedLM\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalBertLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class LegalBertLongForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.bert.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = LegalBertLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_model(save_model_to, attention_window, max_pos):\n",
    "    model = AutoModelForMaskedLM.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased', model_max_length=max_pos)\n",
    "    config = model.config\n",
    "\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.bert.embeddings.position_embeddings.weight.shape\n",
    "    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
    "    config.max_position_embeddings = max_pos\n",
    "    assert max_pos > current_max_pos\n",
    "    # allocate a larger position embedding matrix\n",
    "    new_pos_embed = model.bert.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 2\n",
    "    step = current_max_pos - 2\n",
    "    while k < max_pos - 1:\n",
    "        if k + step < max_pos - 1:\n",
    "            new_pos_embed[k:(k + step)] = model.bert.embeddings.position_embeddings.weight[2:]\n",
    "            k += step\n",
    "        else:\n",
    "            new_pos_embed[k:-1] = model.bert.embeddings.position_embeddings.weight[2:2+max_pos - 1 - k]\n",
    "            k = max_pos - 1\n",
    "    model.bert.embeddings.position_embeddings.weight.data = new_pos_embed\n",
    "    model.bert.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n",
    "\n",
    "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    for i, layer in enumerate(model.bert.encoder.layer):\n",
    "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
    "        longformer_self_attn.query = layer.attention.self.query\n",
    "        longformer_self_attn.key = layer.attention.self.key\n",
    "        longformer_self_attn.value = layer.attention.self.value\n",
    "\n",
    "        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "\n",
    "        layer.attention.self = longformer_self_attn\n",
    "\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting roberta-base into roberta-base-4096\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (16) must match the existing size (510) at non-singleton dimension 0.  Target sizes: [16, 768].  Tensor sizes: [510, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_path)\n\u001b[1;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConverting roberta-base into roberta-base-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_args\u001b[38;5;241m.\u001b[39mmax_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_long_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_model_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pos\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mcreate_long_model\u001b[0;34m(save_model_to, attention_window, max_pos)\u001b[0m\n\u001b[1;32m     17\u001b[0m step \u001b[38;5;241m=\u001b[39m current_max_pos \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m k \u001b[38;5;241m<\u001b[39m max_pos \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mnew_pos_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     20\u001b[0m     k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mposition_embeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m new_pos_embed\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (16) must match the existing size (510) at non-singleton dimension 0.  Target sizes: [16, 768].  Tensor sizes: [510, 768]"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "model_args = ModelArgs()\n",
    "\n",
    "model_path = f'tmp/legalbert-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
    "model, tokenizer = create_long_model(\n",
    "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
