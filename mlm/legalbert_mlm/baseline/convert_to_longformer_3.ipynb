{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook needs to be converted to python file and run with deepspeed filename.py\n",
    "### DeepSpeed needs to have https://github.com/microsoft/DeepSpeed/pull/5780 integrated into deepspeed/ops/op_builder/builder.py\n",
    "### LongformerSelfAttention forward have a line that needs to be changed\n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, :, None, None]\n",
    "### have incorrect shape because the thing that is passed in is [a, 1, 1, b]\n",
    "### so need to change it to \n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, 0, 0, :, None, None]\n",
    "### Another problem occurs where the code in the same function\n",
    "### attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n",
    "### does not check if is_index_masked is None\n",
    "### So this needs to be changed to\n",
    "### if is_index_masked:\n",
    "###     attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import BertForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser, AutoModelForMaskedLM, BertTokenizerFast\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalBertLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class LegalBertLongForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.bert.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = LegalBertLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.bert.encoder.layer):\n",
    "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "model_args = ModelArgs()\n",
    "\n",
    "model_hidden_size = 512\n",
    "train_batch_size = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"tmp\",\n",
    "    max_steps = 3000,\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    # deepspeed=\"ds_config.json\",\n",
    ")\n",
    "training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
    "training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
    "\n",
    "model_path = f'{training_args.output_dir}/legalbert-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                              file_path=args.val_datapath,\n",
    "                              block_size=tokenizer.model_max_length)\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                                    file_path=args.train_datapath,\n",
    "                                    block_size=tokenizer.model_max_length)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=train_dataset, eval_dataset=val_dataset) # , prediction_loss_only=True,)\n",
    "\n",
    "    \"\"\"\n",
    "    ### CANT DO EVALUATE HERE. IT MESSES UP WITH SETTING UP DEEPSPEED FOR TRAINING ### \n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "    \"\"\"\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from tmp/roberta-base-4096\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=4096)\n",
    "model = LegalBertLongForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_save_path = \"tmp_save\"\n",
    "if not os.path.exists(tmp_save_path):\n",
    "    os.mkdir(tmp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-19 14:15:46,406] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
      "Processing zero checkpoint 'tmp_save/global_step3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected checkpoint of type zero stage 3, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.14.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(file, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Trainable fp32 state dict with 274 params 133529658 elements\n",
      "{'bert.embeddings.word_embeddings.weight': <class 'torch.Tensor'>, 'bert.embeddings.position_embeddings.weight': <class 'torch.Tensor'>, 'bert.embeddings.token_type_embeddings.weight': <class 'torch.Tensor'>, 'bert.embeddings.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.embeddings.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.0.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.0.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.1.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.1.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.2.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.2.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.3.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.3.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.4.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.4.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.5.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.5.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.6.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.6.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.7.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.7.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.8.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.8.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.9.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.9.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.10.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.10.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.query.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.query.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.key.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.key.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.value.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.value.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.query_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.query_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.key_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.key_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.value_global.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.self.value_global.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.intermediate.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.intermediate.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.output.dense.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.output.dense.bias': <class 'torch.Tensor'>, 'bert.encoder.layer.11.output.LayerNorm.weight': <class 'torch.Tensor'>, 'bert.encoder.layer.11.output.LayerNorm.bias': <class 'torch.Tensor'>, 'cls.predictions.bias': <class 'torch.Tensor'>, 'cls.predictions.transform.dense.weight': <class 'torch.Tensor'>, 'cls.predictions.transform.dense.bias': <class 'torch.Tensor'>, 'cls.predictions.transform.LayerNorm.weight': <class 'torch.Tensor'>, 'cls.predictions.transform.LayerNorm.bias': <class 'torch.Tensor'>, 'cls.predictions.decoder.weight': <class 'torch.Tensor'>, 'cls.predictions.decoder.bias': <class 'torch.Tensor'>}\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n",
    "\n",
    "state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_save_path) # already on cpu\n",
    "print({k: type(v) for k, v in state_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-19 14:15:48,488] [INFO] [zero_to_fp32.py:570:load_state_dict_from_zero_checkpoint] Extracting fp32 weights\n",
      "Processing zero checkpoint 'tmp_save/global_step3'\n",
      "Detected checkpoint of type zero stage 3, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.14.4\n",
      "Reconstructed Trainable fp32 state dict with 274 params 133529658 elements\n",
      "[2024-08-19 14:15:49,196] [INFO] [zero_to_fp32.py:573:load_state_dict_from_zero_checkpoint] Overwriting model with fp32 weights\n",
      "LegalBertLongForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(4098, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): LegalBertLongSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n",
    "model = LegalBertLongForMaskedLM.from_pretrained(model_path)\n",
    "model = load_state_dict_from_zero_checkpoint(model, tmp_save_path)\n",
    "# loaded_model.load_state_dict(state_dict, strict=False)\n",
    "# loaded_model.load_state_dict(state_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Copying local projection layers into global projection layers ... \n",
      "INFO:__main__:Saving model to model_path_global\n"
     ]
    }
   ],
   "source": [
    "model_path_global = \"model_path_global\"\n",
    "if not os.path.exists(model_path_global):\n",
    "    os.mkdir(model_path_global)\n",
    "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
    "model = copy_proj_layers(model)\n",
    "logger.info(f'Saving model to {model_path_global}')\n",
    "model.save_pretrained(model_path_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from model_path_global\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path_global}')\n",
    "### tokenizer = RobertaTokenizerFast.from_pretrained(model_path_global)\n",
    "model = LegalBertLongForMaskedLM.from_pretrained(model_path_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0126, -0.0295, -0.0126,  ..., -0.0294, -0.0417, -0.0240],\n",
       "        [-0.0580,  0.0049,  0.0112,  ..., -0.0799, -0.0648, -0.0321],\n",
       "        [-0.0230,  0.0208, -0.0591,  ..., -0.0287, -0.0791, -0.0778],\n",
       "        ...,\n",
       "        [-0.0188,  0.0665,  0.0168,  ...,  0.0267, -0.0039, -0.0428],\n",
       "        [-0.0558,  0.0061,  0.0137,  ..., -0.0819,  0.0236,  0.0089],\n",
       "        [-0.0174,  0.0405,  0.0303,  ..., -0.0673, -0.0832,  0.0496]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.word_embeddings.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
