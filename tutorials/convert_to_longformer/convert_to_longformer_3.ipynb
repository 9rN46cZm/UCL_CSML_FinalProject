{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook needs to be converted to python file and run with deepspeed filename.py\n",
    "### DeepSpeed needs to have https://github.com/microsoft/DeepSpeed/pull/5780 integrated into deepspeed/ops/op_builder/builder.py\n",
    "### LongformerSelfAttention forward have a line that needs to be changed\n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, :, None, None]\n",
    "### have incorrect shape because the thing that is passed in is [a, 1, 1, b]\n",
    "### so need to change it to \n",
    "### remove_from_windowed_attention_mask = (attention_mask != 0)[:, 0, 0, :, None, None]\n",
    "### Another problem occurs where the code in the same function\n",
    "### attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n",
    "### does not check if is_index_masked is None\n",
    "### So this needs to be changed to\n",
    "### if is_index_masked:\n",
    "###     attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.roberta.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "model_args = ModelArgs()\n",
    "\n",
    "model_hidden_size = 512\n",
    "train_batch_size = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"tmp\",\n",
    "    max_steps = 3000,\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    # deepspeed=\"ds_config.json\",\n",
    ")\n",
    "training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
    "training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
    "\n",
    "model_path = f'{training_args.output_dir}/roberta-base-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path):\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                              file_path=args.val_datapath,\n",
    "                              block_size=tokenizer.model_max_length)\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                                    file_path=args.train_datapath,\n",
    "                                    block_size=tokenizer.model_max_length)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    trainer = Trainer(model=model, args=args, data_collator=data_collator,\n",
    "                      train_dataset=train_dataset, eval_dataset=val_dataset) # , prediction_loss_only=True,)\n",
    "\n",
    "    \"\"\"\n",
    "    ### CANT DO EVALUATE HERE. IT MESSES UP WITH SETTING UP DEEPSPEED FOR TRAINING ### \n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "    \"\"\"\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from tmp/roberta-base-4096\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_save_path = \"tmp_save\"\n",
    "if not os.path.exists(tmp_save_path):\n",
    "    os.mkdir(tmp_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing zero checkpoint 'tmp_save/global_step3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected checkpoint of type zero stage 3, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.14.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/deepspeed/utils/zero_to_fp32.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(file, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Trainable fp32 state dict with 274 params 148711257 elements\n",
      "{'roberta.embeddings.word_embeddings.weight': <class 'torch.Tensor'>, 'roberta.embeddings.position_embeddings.weight': <class 'torch.Tensor'>, 'roberta.embeddings.token_type_embeddings.weight': <class 'torch.Tensor'>, 'roberta.embeddings.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.embeddings.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.0.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.1.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.2.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.3.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.4.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.5.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.6.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.7.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.8.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.9.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.10.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.query.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.query.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.key.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.key.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.value.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.value.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.query_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.query_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.key_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.key_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.value_global.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.self.value_global.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.attention.output.LayerNorm.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.intermediate.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.intermediate.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.output.dense.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.output.dense.bias': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.output.LayerNorm.weight': <class 'torch.Tensor'>, 'roberta.encoder.layer.11.output.LayerNorm.bias': <class 'torch.Tensor'>, 'lm_head.bias': <class 'torch.Tensor'>, 'lm_head.dense.weight': <class 'torch.Tensor'>, 'lm_head.dense.bias': <class 'torch.Tensor'>, 'lm_head.layer_norm.weight': <class 'torch.Tensor'>, 'lm_head.layer_norm.bias': <class 'torch.Tensor'>, 'lm_head.decoder.weight': <class 'torch.Tensor'>, 'lm_head.decoder.bias': <class 'torch.Tensor'>}\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n",
    "\n",
    "state_dict = get_fp32_state_dict_from_zero_checkpoint(tmp_save_path) # already on cpu\n",
    "print({k: type(v) for k, v in state_dict.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-12 22:53:29,523] [INFO] [zero_to_fp32.py:570:load_state_dict_from_zero_checkpoint] Extracting fp32 weights\n",
      "Processing zero checkpoint 'tmp_save/global_step3'\n",
      "Detected checkpoint of type zero stage 3, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.14.4\n",
      "Reconstructed Trainable fp32 state dict with 274 params 148711257 elements\n",
      "[2024-08-12 22:53:30,031] [INFO] [zero_to_fp32.py:573:load_state_dict_from_zero_checkpoint] Overwriting model with fp32 weights\n",
      "RobertaLongForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaLongSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path)\n",
    "model = load_state_dict_from_zero_checkpoint(model, tmp_save_path)\n",
    "# loaded_model.load_state_dict(state_dict, strict=False)\n",
    "# loaded_model.load_state_dict(state_dict)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Copying local projection layers into global projection layers ... \n",
      "INFO:__main__:Saving model to model_path_global\n"
     ]
    }
   ],
   "source": [
    "model_path_global = \"model_path_global\"\n",
    "if not os.path.exists(model_path_global):\n",
    "    os.mkdir(model_path_global)\n",
    "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
    "model = copy_proj_layers(model)\n",
    "logger.info(f'Saving model to {model_path_global}')\n",
    "model.save_pretrained(model_path_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from model_path_global\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path_global}')\n",
    "### tokenizer = RobertaTokenizerFast.from_pretrained(model_path_global)\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
       "        [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
       "        [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
       "        ...,\n",
       "        [ 0.0304,  0.0504, -0.0307,  ...,  0.0377,  0.0096,  0.0084],\n",
       "        [ 0.0623, -0.0596,  0.0307,  ..., -0.0920,  0.1080, -0.0183],\n",
       "        [ 0.1259, -0.0145,  0.0332,  ...,  0.0121,  0.0342,  0.0168]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.embeddings.word_embeddings.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
