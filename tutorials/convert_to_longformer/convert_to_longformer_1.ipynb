{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wikitext_103_raw_v1 = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "\n",
    "import os\n",
    "def save_to_txt(ds_mode, save_fn):\n",
    "    tt = ''.join([t['text'] for t in ds_mode])\n",
    "    with open(save_fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(tt)\n",
    "if not os.path.exists('wikitext-103-raw'):\n",
    "    os.mkdir('wikitext-103-raw')\n",
    "save_to_txt(wikitext_103_raw_v1['train'].select(range(1000)), \"wikitext-103-raw/wiki.train.raw\")\n",
    "save_to_txt(wikitext_103_raw_v1['validation'], \"wikitext-103-raw/wiki.valid.raw\")\n",
    "save_to_txt(wikitext_103_raw_v1['test'], \"wikitext-103-raw/wiki.test.raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.roberta.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = RobertaLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_model(save_model_to, attention_window, max_pos):\n",
    "    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', model_max_length=max_pos)\n",
    "    config = model.config\n",
    "\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n",
    "    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
    "    config.max_position_embeddings = max_pos\n",
    "    assert max_pos > current_max_pos\n",
    "    # allocate a larger position embedding matrix\n",
    "    new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 2\n",
    "    step = current_max_pos - 2\n",
    "    while k < max_pos - 1:\n",
    "        new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n",
    "        k += step\n",
    "    model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n",
    "    model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos)\n",
    "\n",
    "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
    "        longformer_self_attn.query = layer.attention.self.query\n",
    "        longformer_self_attn.key = layer.attention.self.key\n",
    "        longformer_self_attn.value = layer.attention.self.value\n",
    "\n",
    "        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "\n",
    "        layer.attention.self = longformer_self_attn\n",
    "\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        layer.attention.self.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        layer.attention.self.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        layer.attention.self.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting roberta-base into roberta-base-4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is model certainly initialized? RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "is the problem already here? torch.Size([514, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "INFO:__main__:saving model to tmp/roberta-base-4096\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "model_args = ModelArgs()\n",
    "\n",
    "model_path = f'tmp/roberta-base-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
    "model, tokenizer = create_long_model(\n",
    "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
