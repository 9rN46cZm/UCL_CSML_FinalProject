{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate semantic chunking on contract nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"..\"\n",
    "\n",
    "id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def contract_nli_iterator(data):\n",
    "    documents, labels = data['documents'], data['labels']\n",
    "    for document in documents:\n",
    "        id = document['id']\n",
    "        file_name = document['file_name']\n",
    "        text = document['text']\n",
    "        spans = document['spans']\n",
    "        annotation_sets = document['annotation_sets']\n",
    "        document_type = document['document_type']\n",
    "        url = document['url']\n",
    "        for annotation_id, annotation_content in annotation_sets[0]['annotations'].items():\n",
    "            hypothesis = labels[annotation_id]['hypothesis']\n",
    "            choice = annotation_content['choice']\n",
    "            yield {\n",
    "                \"id\": id,\n",
    "                \"file_name\": file_name,\n",
    "                \"text\": text,\n",
    "                \"spans\": spans,\n",
    "                \"document_type\": document_type,\n",
    "                \"url\": url,\n",
    "                \"hypothesis\": hypothesis,\n",
    "                \"labels\": label2id[choice],\n",
    "            }            \n",
    "base_filepath = os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli\")\n",
    "train_filepath = os.path.join(base_filepath, \"train.json\")\n",
    "validation_filepath = os.path.join(base_filepath, \"dev.json\")\n",
    "test_filepath = os.path.join(base_filepath, \"test.json\")\n",
    "with open(train_filepath) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(validation_filepath) as f:\n",
    "    validation_data = json.load(f)\n",
    "with open(test_filepath) as f:\n",
    "    test_data = json.load(f)\n",
    "data = {\n",
    "    \"train\": Dataset.from_generator(lambda: contract_nli_iterator(train_data)),\n",
    "    \"validation\": Dataset.from_generator(lambda: contract_nli_iterator(validation_data)),\n",
    "    \"test\": Dataset.from_generator(lambda: contract_nli_iterator(test_data)),\n",
    "}\n",
    "contract_nli_dataset = DatasetDict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Chunking Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device {device}\")\n",
    "vanilla_chunking_facebook_bart_large_mnli_autotokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "vanilla_chunking_facebook_bart_large_mnli_automodel_for_sequence_classification = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli').to(device)\n",
    "vanilla_chunking_bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "vanilla_chunking_bart_large_mnli_label2id = {v: k for k, v in vanilla_chunking_bart_large_mnli_id2labal.items()}\n",
    "vanilla_chunking_parser = SentenceSplitter(chunk_size=300, chunk_overlap=50, tokenizer=lambda x: vanilla_chunking_facebook_bart_large_mnli_autotokenizer(x)['input_ids'])\n",
    "vanilla_chunking_softmax_f = nn.Softmax(dim=1)\n",
    "\n",
    "def calculate_prob_with_vanilla_chunking(premise, hypothesis):\n",
    "    ans = None\n",
    "    with torch.no_grad():\n",
    "        premise = e['text']\n",
    "        hypothesis = e['hypothesis']\n",
    "        premise_chunks = vanilla_chunking_parser.get_nodes_from_documents([Document(text=premise)])\n",
    "        premise_chunks = [e.text for e in premise_chunks]\n",
    "        e_tokens = vanilla_chunking_facebook_bart_large_mnli_autotokenizer.batch_encode_plus([[premise_chunk, hypothesis] for premise_chunk in premise_chunks], \\\n",
    "                                                                                             return_tensors='pt', padding='longest', truncation='only_first')\n",
    "        e_tokens = e_tokens['input_ids'].detach().cpu()\n",
    "        logits = vanilla_chunking_facebook_bart_large_mnli_automodel_for_sequence_classification(e_tokens.to(device)).logits\n",
    "        averaged_probs = torch.mean(vanilla_chunking_softmax_f(logits), dim=0)\n",
    "        # ans = torch.argmax(averaged_probs)\n",
    "        ans = averaged_probs\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate vanilla chunking on contract nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_nli_dataset_test = contract_nli_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading semantic chunking embedding model to cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(\"loading semantic chunking embedding model to cuda:1\")\n",
    "semantic_chunking_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunking_splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=semantic_chunking_embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2091 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3207 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2091/2091 [54:34<00:00,  1.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_chunking_bart_label_to_contract_nli_label(id):\n",
    "    if id == 0: return 1\n",
    "    elif id == 1: return 2\n",
    "    elif id == 2: return 0\n",
    "    else: return None\n",
    "\n",
    "semantic_chunking_softmax_f = nn.Softmax(dim=1)\n",
    "semantic_chunking_chunk_record_test = dict()\n",
    "semantic_chunking_averaged_probs_record_test = torch.tensor([[0.0, 0.0, 0.0] for _ in range(len(contract_nli_dataset_test))])\n",
    "semantic_chunking_accuracy_record_test = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_test))])\n",
    "with torch.no_grad():\n",
    "    for i, e in tqdm(enumerate(contract_nli_dataset_test), total=len(contract_nli_dataset_test)):\n",
    "        premise = e['text']\n",
    "        hypothesis = e['hypothesis']\n",
    "        premise_chunks = semantic_chunking_chunk_record_test.get(e['file_name'], None)\n",
    "        if premise_chunks is None:\n",
    "            premise_chunks = semantic_chunking_splitter.get_nodes_from_documents([Document(text=premise)])\n",
    "            premise_chunks = [e.text for e in premise_chunks]\n",
    "            semantic_chunking_chunk_record_test[e['file_name']] = premise_chunks\n",
    "        premise_chunk_prob_results = torch.tensor([[0.0, 0.0, 0.0] for _ in range(len(premise_chunks))])\n",
    "        for j, premise_chunk in enumerate(premise_chunks):\n",
    "            premise_chunk_prob = calculate_prob_with_vanilla_chunking(premise_chunk, hypothesis)\n",
    "            premise_chunk_prob_results[j] = premise_chunk_prob\n",
    "        averaged_probs = torch.mean(premise_chunk_prob_results, dim=0)\n",
    "        semantic_chunking_averaged_probs_record_test[i] = averaged_probs\n",
    "        ans = torch.argmax(averaged_probs)\n",
    "        label = e['labels']\n",
    "        semantic_chunking_accuracy_record_test[i] = 1 if label == semantic_chunking_bart_label_to_contract_nli_label(ans) else 0\n",
    "print(torch.mean(semantic_chunking_accuracy_record_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181 341 446]\n",
      " [ 14 117  89]\n",
      " [ 80 467 356]]\n"
     ]
    }
   ],
   "source": [
    "### Get confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "\n",
    "for i, e in enumerate(contract_nli_dataset_test):\n",
    "    averaged_probs = semantic_chunking_averaged_probs_record_test[i]\n",
    "    ans = torch.argmax(averaged_probs)\n",
    "    ans = semantic_chunking_bart_label_to_contract_nli_label(ans)\n",
    "    preds.append(ans)\n",
    "    label = e['labels']\n",
    "    labels.append(label)\n",
    "\n",
    "conf_mat = confusion_matrix(labels, preds)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the data\n",
    "import pickle\n",
    "\n",
    "save_dir_path = \"../ignored_dir/results/semantic_chunking_on_contract_nli\"\n",
    "if not os.path.exists(save_dir_path):\n",
    "    os.mkdir(save_dir_path)\n",
    "readme_path = os.path.join(save_dir_path, \"README.txt\")\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write('{0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}')\n",
    "conf_mat_path = os.path.join(save_dir_path, \"conf_mat.pkl\")\n",
    "conf_mat_filehandler = open(conf_mat_path, 'wb') \n",
    "pickle.dump(conf_mat, conf_mat_filehandler)\n",
    "conf_mat_filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181 341 446]\n",
      " [ 14 117  89]\n",
      " [ 80 467 356]]\n"
     ]
    }
   ],
   "source": [
    "### Test load\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "save_dir_path = \"../ignored_dir/results/semantic_chunking_on_contract_nli\"\n",
    "conf_mat_path = os.path.join(save_dir_path, \"conf_mat.pkl\")\n",
    "\n",
    "with open(conf_mat_path, 'rb') as f:\n",
    "    conf_mat_loaded = pickle.load(f)\n",
    "print(conf_mat_loaded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make table\n",
    "conf_mat = conf_mat_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3127690100430416\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum([conf_mat[i][i] for i in range(3)]) / sum(sum(conf_mat))\n",
    "print(f\"accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entailment', 'Contradiction', 'NotMentioned']\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "labels = [id2label[i] for i in range(3)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = \"\"\"\\\\begin{center}\n",
    "\\\\begin{tabular}{ |c|c|c|c|c| } \n",
    "\\hline\n",
    "& \\multicolumn{4}{|c|}{pred} \\\\\\\\\n",
    "\\hline\n",
    "\\multirow{4}{2em}{true} &  & \"\"\" + f\"{labels[0]} & {labels[1]} & {labels[2]} \\\\\\\\\\n\" \\\n",
    "+ \"\"\"\\cline{2-5}\n",
    "& \"\"\" + f\"{labels[0]} & {conf_mat[0][0]} & {conf_mat[0][1]} & {conf_mat[0][2]}\\\\\\\\\\n\" \\\n",
    "+ \"\"\"\\cline{2-5}\n",
    "& \"\"\" + f\"{labels[1]} & {conf_mat[1][0]} & {conf_mat[1][1]} & {conf_mat[1][2]}\\\\\\\\\\n\" \\\n",
    "+ \"\"\"\\cline{2-5}\n",
    "& \"\"\" + f\"{labels[2]} & {conf_mat[2][0]} & {conf_mat[2][1]} & {conf_mat[2][2]}\\\\\\\\\\n\" \\\n",
    "+ \"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{center}\n",
      "\\begin{tabular}{ |c|c|c|c|c| } \n",
      "\\hline\n",
      "& \\multicolumn{4}{|c|}{pred} \\\\\n",
      "\\hline\n",
      "\\multirow{4}{2em}{true} &  & Entailment & Contradiction & NotMentioned \\\\\n",
      "\\cline{2-5}\n",
      "& Entailment & 181 & 341 & 446\\\\\n",
      "\\cline{2-5}\n",
      "& Contradiction & 14 & 117 & 89\\\\\n",
      "\\cline{2-5}\n",
      "& NotMentioned & 80 & 467 & 356\\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{center}\n"
     ]
    }
   ],
   "source": [
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two category result\n",
      "tensor(0.8867)\n",
      "220 2091\n",
      "{'NotMentioned': 903, 'Entailment': 968, 'Contradiction': 220}\n",
      "[[1854   17]\n",
      " [ 220    0]]\n",
      "three catogory with thresholding result\n",
      "tensor(0.3462)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Results analysis: Turn into two categories.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\"\"\"\n",
    "\n",
    "# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "\n",
    "\"\"\"\n",
    "tmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_test))])\n",
    "contradiction_cnt = 0\n",
    "label_sum = dict()\n",
    "labels_test = []\n",
    "preds_test = []\n",
    "for i, e in enumerate(contract_nli_dataset_test):\n",
    "    averaged_probs = semantic_chunking_averaged_probs_record_test[i]\n",
    "    ans = 1 if averaged_probs[0] > 0.9 else 0\n",
    "    # ans = torch.argmax(averaged_probs)\n",
    "    label = e['labels']\n",
    "    label_sum[id2label[label]] = label_sum.get(id2label[label], 0) + 1\n",
    "    # ans = bart_label_to_contract_nli_label(ans)\n",
    "    ans = 1 if ans == 1 else 0\n",
    "    label = 1 if label == 1 else 0\n",
    "    tmp[i] = 1 if ans == label else 0\n",
    "    labels_test.append(label)\n",
    "    preds_test.append(ans)\n",
    "    if label == 1:\n",
    "        contradiction_cnt += 1\n",
    "conf_mat_test = confusion_matrix(labels_test, preds_test)\n",
    "print(\"two category result\")\n",
    "print(torch.mean(tmp))\n",
    "print(contradiction_cnt, len(contract_nli_dataset_test))\n",
    "print(label_sum)\n",
    "print(conf_mat_test)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Results analysis: Set threshold for neutral as well.\n",
    "\"\"\"\n",
    "\n",
    "# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "\n",
    "\"\"\"\n",
    "tmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_test))])\n",
    "contradiction_cnt = 0\n",
    "label_sum = dict()\n",
    "labels_test = []\n",
    "preds_test = []\n",
    "for i, e in enumerate(contract_nli_dataset_test):\n",
    "    averaged_probs = semantic_chunking_averaged_probs_record_test[i]\n",
    "    ans = semantic_chunking_bart_label_to_contract_nli_label(ans)\n",
    "    label = e['labels']\n",
    "    labels_test.append(label)\n",
    "    preds_test.append(ans)\n",
    "    tmp[i] = 1 if ans == label else 0\n",
    "print(\"three catogory with thresholding result\")\n",
    "print(torch.mean(tmp))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract_nli_dataset_all = datasets.concatenate_datasets([contract_nli_dataset['train'], contract_nli_dataset['validation'], contract_nli_dataset['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparser = SentenceSplitter(chunk_size=300, chunk_overlap=50, tokenizer=lambda x: facebook_bart_large_mnli_autotokenizer(x)['input_ids'])\\n#contract_nodes = parser.get_nodes_from_documents(contract_documents, show_progress=True)\\n\\nchunk_record_all = dict()\\nsoftmax_f_all = nn.Softmax(dim=1)\\naveraged_probs_record_all = torch.tensor([[0.0, 0.0, 0.0] for _ in range(len(contract_nli_dataset_all))])\\naccuracy_record_all = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\\nwith torch.no_grad():\\n    for i, e in tqdm(enumerate(contract_nli_dataset_all), total=len(contract_nli_dataset_all)):\\n        premise = e['text']\\n        hypothesis = e['hypothesis']\\n        premise_chunks = chunk_record_all.get(e['file_name'], None)\\n        if premise_chunks is None:\\n            premise_chunks = parser.get_nodes_from_documents([Document(text=premise)])\\n            premise_chunks = [e.text for e in premise_chunks]\\n            chunk_record_all[e['file_name']] = premise_chunks\\n        e_tokens = facebook_bart_large_mnli_autotokenizer.batch_encode_plus([[premise_chunk, hypothesis] for premise_chunk in premise_chunks], return_tensors='pt', padding='longest', truncation='only_first')\\n        e_tokens = e_tokens['input_ids'].detach().cpu()\\n        logits = facebook_bart_large_mnli_automodel_for_sequence_classification(e_tokens.to(device)).logits\\n        averaged_probs = torch.mean(softmax_f(logits), dim=0)\\n        averaged_probs_record_all[i] = averaged_probs\\n        ans = torch.argmax(averaged_probs)\\n        label = e['labels']\\n        accuracy_record_all[i] = 1 if label == bart_label_to_contract_nli_label(ans) else 0\\nprint(torch.mean(accuracy_record_all))\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try all data to see if results persist. \n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "parser = SentenceSplitter(chunk_size=300, chunk_overlap=50, tokenizer=lambda x: facebook_bart_large_mnli_autotokenizer(x)['input_ids'])\n",
    "#contract_nodes = parser.get_nodes_from_documents(contract_documents, show_progress=True)\n",
    "\n",
    "chunk_record_all = dict()\n",
    "softmax_f_all = nn.Softmax(dim=1)\n",
    "averaged_probs_record_all = torch.tensor([[0.0, 0.0, 0.0] for _ in range(len(contract_nli_dataset_all))])\n",
    "accuracy_record_all = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\n",
    "with torch.no_grad():\n",
    "    for i, e in tqdm(enumerate(contract_nli_dataset_all), total=len(contract_nli_dataset_all)):\n",
    "        premise = e['text']\n",
    "        hypothesis = e['hypothesis']\n",
    "        premise_chunks = chunk_record_all.get(e['file_name'], None)\n",
    "        if premise_chunks is None:\n",
    "            premise_chunks = parser.get_nodes_from_documents([Document(text=premise)])\n",
    "            premise_chunks = [e.text for e in premise_chunks]\n",
    "            chunk_record_all[e['file_name']] = premise_chunks\n",
    "        e_tokens = facebook_bart_large_mnli_autotokenizer.batch_encode_plus([[premise_chunk, hypothesis] for premise_chunk in premise_chunks], return_tensors='pt', padding='longest', truncation='only_first')\n",
    "        e_tokens = e_tokens['input_ids'].detach().cpu()\n",
    "        logits = facebook_bart_large_mnli_automodel_for_sequence_classification(e_tokens.to(device)).logits\n",
    "        averaged_probs = torch.mean(softmax_f(logits), dim=0)\n",
    "        averaged_probs_record_all[i] = averaged_probs\n",
    "        ans = torch.argmax(averaged_probs)\n",
    "        label = e['labels']\n",
    "        accuracy_record_all[i] = 1 if label == bart_label_to_contract_nli_label(ans) else 0\n",
    "print(torch.mean(accuracy_record_all))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\\n# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\\n\\ntmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\\ncontradiction_cnt = 0\\ncontradiction_correct_cnt = 0\\nlabel_sum = dict()\\nfor i, e in enumerate(contract_nli_dataset_all):\\n    averaged_probs = averaged_probs_record_all[i]\\n    ans = 1 if averaged_probs[0] > 0.6 else 0\\n    # ans = torch.argmax(averaged_probs)\\n    label = e[\\'labels\\']\\n    label_sum[id2label[label]] = label_sum.get(id2label[label], 0) + 1\\n    # ans = bart_label_to_contract_nli_label(ans)\\n    ans = 1 if ans == 1 else 0\\n    label = 1 if label == 1 else 0\\n    tmp[i] = 1 if ans == label else 0\\n    if label == 1:\\n        contradiction_cnt += 1\\n    if ans == label == 1:\\n        contradiction_correct_cnt += 1\\nprint(torch.mean(tmp))\\nprint(f\"contradiction correct cnt: {contradiction_correct_cnt}\")\\nprint(f\"contradiction cnt: {contradiction_cnt}\")\\nprint(f\"contradiction hit rate: {contradiction_correct_cnt / contradiction_cnt}\")\\nprint(label_sum)\\n\\n\"\"\"\\nResults analysis: Set threshold for neutral as well.\\n\"\"\"\\n\\n# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\\n# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\\n\\ntmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\\ncontradiction_cnt = 0\\nlabel_sum = dict()\\nfor i, e in enumerate(contract_nli_dataset_all):\\n    averaged_probs = averaged_probs_record_all[i]\\n    ans = 1 if averaged_probs[0] > 0.9 else (0 if averaged_probs[1] > 0.9 else 2)\\n    # ans = torch.argmax(averaged_probs)\\n    label = e[\\'labels\\']\\n    tmp[i] = 1 if ans == label else 0\\nprint(torch.mean(tmp))\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Results analysis: Turn into two categories.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "\n",
    "# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "\n",
    "tmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\n",
    "contradiction_cnt = 0\n",
    "contradiction_correct_cnt = 0\n",
    "label_sum = dict()\n",
    "for i, e in enumerate(contract_nli_dataset_all):\n",
    "    averaged_probs = averaged_probs_record_all[i]\n",
    "    ans = 1 if averaged_probs[0] > 0.6 else 0\n",
    "    # ans = torch.argmax(averaged_probs)\n",
    "    label = e['labels']\n",
    "    label_sum[id2label[label]] = label_sum.get(id2label[label], 0) + 1\n",
    "    # ans = bart_label_to_contract_nli_label(ans)\n",
    "    ans = 1 if ans == 1 else 0\n",
    "    label = 1 if label == 1 else 0\n",
    "    tmp[i] = 1 if ans == label else 0\n",
    "    if label == 1:\n",
    "        contradiction_cnt += 1\n",
    "    if ans == label == 1:\n",
    "        contradiction_correct_cnt += 1\n",
    "print(torch.mean(tmp))\n",
    "print(f\"contradiction correct cnt: {contradiction_correct_cnt}\")\n",
    "print(f\"contradiction cnt: {contradiction_cnt}\")\n",
    "print(f\"contradiction hit rate: {contradiction_correct_cnt / contradiction_cnt}\")\n",
    "print(label_sum)\n",
    "\n",
    "\"\"\"\n",
    "Results analysis: Set threshold for neutral as well.\n",
    "\"\"\"\n",
    "\n",
    "# bart_large_mnli_id2labal = {0: \"contradiction\", 1: \"neutral\", 2: \"entailment\"}\n",
    "# id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMentioned\"}\n",
    "\n",
    "tmp = torch.tensor([0.0 for _ in range(len(contract_nli_dataset_all))])\n",
    "contradiction_cnt = 0\n",
    "label_sum = dict()\n",
    "for i, e in enumerate(contract_nli_dataset_all):\n",
    "    averaged_probs = averaged_probs_record_all[i]\n",
    "    ans = 1 if averaged_probs[0] > 0.9 else (0 if averaged_probs[1] > 0.9 else 2)\n",
    "    # ans = torch.argmax(averaged_probs)\n",
    "    label = e['labels']\n",
    "    tmp[i] = 1 if ans == label else 0\n",
    "print(torch.mean(tmp))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
