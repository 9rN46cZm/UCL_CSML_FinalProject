{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "1. ~~Tokenizer usage is not consistent~~\n",
    "2. batch_encode_plus, what does it do?\n",
    "3. currently the pdf text is stitched together with '\\n' from each page\n",
    "4. Currently the implementation is contract nodes saved on disk. In practice it should be law nodes saved on disk and contract passed as parameter of function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvcc --version\n",
    "#!which pip\n",
    "#!which pip3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://artifactory-eo.corp.qbe.com/artifactory/api/pypi/gl-pypi-remote/simple\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading https://artifactory-eo.corp.qbe.com/artifactory/api/pypi/gl-pypi-remote/packages/packages/6d/64/cce82bddb80c0b0f5c703bbdafa94bfb69a1c5ad7a79cff00b482468f0d3/nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19189 sha256=8c4a477905b04b4781793b724c25f47760d29eb183e4f4c2db2ae3d664ce23d1\n",
      "  Stored in directory: /home/yan_xu_uk_qbe_com/.cache/pip/wheels/27/51/23/64e7f2a6c0515f7568fd7b07eb19b54e498ceab83b8d24ab97\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "#!pip install ipywidgets\n",
    "#!pip install llama-index\n",
    "#!pip install nvidia-ml-py3\n",
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pynvml\n",
    "import re\n",
    "import shutil\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/yan_xu_uk_qbe_com/.cache/huggingface/modules/datasets_modules/datasets/pile-of-law--pile-of-law/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/yan_xu_uk_qbe_com/.cache/huggingface/datasets/pile-of-law___pile-of-law/atticus_contracts/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60\n",
      "Found cached dataset pile-of-law (/home/yan_xu_uk_qbe_com/.cache/huggingface/datasets/pile-of-law___pile-of-law/atticus_contracts/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60)\n",
      "Loading Dataset info from /home/yan_xu_uk_qbe_com/.cache/huggingface/datasets/pile-of-law___pile-of-law/atticus_contracts/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4092a63b124cc8a585974a17fed33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contracts = datasets.load_dataset('pile-of-law/pile-of-law', 'atticus_contracts', split='train+validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_documents = [Document(text=t) for t in contracts[:1]['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_bart_large_mnli_autotokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter(chunk_size=300, chunk_overlap=50, tokenizer=lambda x: facebook_bart_large_mnli_autotokenizer(x)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f002dd6acff04d87874746e655d59186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7786 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This gives a warning\n",
    "Token indices sequence length is longer than the specified maximum sequence length for this model (7786 > 1024). Running this sequence through the model will result in indexing errors\n",
    "\n",
    "This is potentially due to facebook_bart_large_mnli_autotokenizer complaining, but doesn't impact the function here.\n",
    "'''\n",
    "contract_nodes = parser.get_nodes_from_documents(contract_documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE TO SAVE OBJECTS ONTO DISK ###\n",
    "\n",
    "TOP_FOLDER = TOP_FOLDER_PATH = 'pkls'\n",
    "if not os.path.exists(TOP_FOLDER):\n",
    "    os.mkdir(TOP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET UP SUBFOLDER\n",
    "\n",
    "PROJECT_FOLDER = \"basic\"\n",
    "PROJECT_FOLDER_PATH = os.path.join(TOP_FOLDER_PATH, PROJECT_FOLDER)\n",
    "\n",
    "assert os.path.exists(TOP_FOLDER_PATH), f\"Top folder path {TOP_FOLDER_PATH} does not exist.\"\n",
    "if not os.path.exists(PROJECT_FOLDER_PATH):\n",
    "    os.mkdir(PROJECT_FOLDER_PATH)\n",
    "\n",
    "### reset folder\n",
    "# shutil.rmtree(PROJECT_FOLDER_PATH)\n",
    "# os.mkdir(PROJECT_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save contract nodes\n",
    "SUBFOLDER = \"contract-nodes\"\n",
    "sub_folder_path = os.path.join(PROJECT_FOLDER_PATH, SUBFOLDER)\n",
    "os.makedirs(sub_folder_path, exist_ok=True)\n",
    "for i, node in enumerate(contract_nodes):\n",
    "    file_path = os.path.join(sub_folder_path, f\"node_{i}.pkl\")\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(node, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nloaded_nodes = [None] * len(os.listdir(sub_folder_path))\\nfor file_name in os.listdir(sub_folder_path):\\n    file_path = os.path.join(sub_folder_path, file_name)\\n    node_idx = int(file_name[:-4].split('_')[-1])\\n    with open(file_path, 'rb') as f:\\n        node = pickle.load(f)\\n        print(node)\\n        break\\n        node = np.load(f, allow_pickle=True)\\n        loaded_nodes[node_idx] = node\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### verify saving\n",
    "\n",
    "\"\"\"\n",
    "loaded_nodes = [None] * len(os.listdir(sub_folder_path))\n",
    "for file_name in os.listdir(sub_folder_path):\n",
    "    file_path = os.path.join(sub_folder_path, file_name)\n",
    "    node_idx = int(file_name[:-4].split('_')[-1])\n",
    "    with open(file_path, 'rb') as f:\n",
    "        node = pickle.load(f)\n",
    "        print(node)\n",
    "        break\n",
    "        node = np.load(f, allow_pickle=True)\n",
    "        loaded_nodes[node_idx] = node\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_act_reader = PdfReader(\"card_act.pdf\")\n",
    "card_act_number_of_pages = len(card_act_reader.pages)\n",
    "card_act_text = '\\n'.join(page.extract_text() for page in card_act_reader.pages)\n",
    "card_act_document = Document(text=card_act_text)\n",
    "card_act_nodes = parser.get_nodes_from_documents([card_act_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook/BART-MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# download model and tokenizer\n",
    "facebook_bart_large_mnli_automodel_for_sequence_classification = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_nodes_folder_path = os.path.join(PROJECT_FOLDER_PATH, \"contract-nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_filenames_pattern = re.compile(\"^(node_[0-9]+.pkl)$\")\n",
    "contract_nodes_filenames = [filename for filename in os.listdir(contract_nodes_folder_path) if nodes_filenames_pattern.match(filename)]\n",
    "contract_nodes_filenames = sorted(contract_nodes_filenames, key=lambda x: int(x.split('.')[0].split('_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_nodes_loaded = []\n",
    "contract_nodes_folderpath = os.path.join(PROJECT_FOLDER, \"contract-nodes\")\n",
    "for i, contract_node_filename in enumerate(contract_nodes_filenames):\n",
    "    assert int(contract_node_filename.split('.')[0].split('_')[-1]) == i, \"\"\n",
    "    contract_node_filepath = os.path.join(contract_nodes_folder_path, contract_node_filename)\n",
    "    with open(contract_node_filepath, 'rb') as f:\n",
    "        contract_node = pickle.load(f)\n",
    "        contract_nodes_loaded.append(contract_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [08:58<00:00, 14.97s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    chunk_size = 20\n",
    "    law_nodes_loaded = card_act_nodes\n",
    "    law_nodes_loaded_chunks = [law_nodes_loaded[i * chunk_size : min(len(law_nodes_loaded), (i + 1) * chunk_size)] for i in range(math.ceil(len(law_nodes_loaded) / chunk_size))]\n",
    "    for i, contract_node in tqdm(enumerate(contract_nodes_loaded), total=len(contract_nodes_loaded)):\n",
    "        for j, law_nodes_chunk in enumerate(law_nodes_loaded_chunks):\n",
    "            prop = facebook_bart_large_mnli_autotokenizer.batch_encode_plus([[law_node.text, contract_node.text] for law_node in law_nodes_chunk], return_tensors='pt', padding='longest', truncation='only_first')['input_ids']\n",
    "            # prop = facebook_bart_large_mnli_autotokenizer.encode(law_node.text, contract_node.text, return_tensors='pt', truncation='only_first').expand(prop.shape[0] * 20, prop.shape[1])\n",
    "            prop = torch.Tensor(prop)\n",
    "            prop_gpu = prop.to(device)\n",
    "            logits = facebook_bart_large_mnli_automodel_for_sequence_classification(prop_gpu).logits\n",
    "            probs = logits.softmax(dim=1)\n",
    "            contradiction_probs = probs[:, 0]\n",
    "            for k, l in enumerate(range(j * chunk_size, min(len(law_nodes_loaded_chunks), (j + 1) * chunk_size))):\n",
    "                record[(i, l)] = contradiction_probs[k]\n",
    "            del prop\n",
    "            del prop_gpu\n",
    "            del logits\n",
    "            del probs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contradiction_score = sum(record.values()) / len(record.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3208, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(contradiction_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
