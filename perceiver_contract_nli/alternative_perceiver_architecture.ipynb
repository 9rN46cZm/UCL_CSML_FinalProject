{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the default perceiver creates 2048 hidden vectors. For long input, the perceiver needs to be able to find the <sep> token accurately. \n",
    "This is obviously not done in the cross attention step as the tokens for the entire input are added together. The only possibility for the model\n",
    "to find the <sep> token and use its information is if the self-attentions somehow finds the existence of the <sep> token and use it accordingly.\n",
    "\n",
    "Since the Perceiver probably does not have special ability to do this find <sep> in long context task, it might be better to solve this problem\n",
    "for it by using another Perceiver to learn query vectors with hypothesis as input. These query vectors, which are equivalent to the initialized\n",
    "hidden vectors for a perceiver model, does cross attention with the premise and then goes from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PerceiverModel, PerceiverPreTrainedModel, PerceiverForSequenceClassification, PerceiverConfig\n",
    "from transformers.models.perceiver.modeling_perceiver import PerceiverTextPreprocessor, PerceiverClassificationDecoder, PreprocessorType, PostprocessorType, PerceiverEmbeddings, \\\n",
    "    PerceiverEncoder, PerceiverLayer, PerceiverAttention, PerceiverMLP, PerceiverSelfAttention, PerceiverSelfOutput, PerceiverClassifierOutput, PerceiverModelOutput, \\\n",
    "    _CONFIG_FOR_DOC, PERCEIVER_INPUTS_DOCSTRING, BaseModelOutputWithCrossAttentions, apply_chunking_to_forward\n",
    "from transformers.utils import add_start_docstrings_to_model_forward, replace_return_docstrings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from typing import Dict, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from modeling_perceiver.py from transformers library\n",
    "\n",
    "class CustomPerceiverEmbeddingGenerator(PerceiverModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        decoder=None,\n",
    "        input_preprocessor: PreprocessorType = None,\n",
    "        output_postprocessor: PostprocessorType = None,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # self.input_preprocessor = input_preprocessor\n",
    "        self.input_preprocessor = PerceiverTextPreprocessor(config)\n",
    "        self.output_postprocessor = output_postprocessor\n",
    "        self.embeddings = PerceiverEmbeddings(config)\n",
    "        self.encoder = PerceiverEncoder(\n",
    "            config, kv_dim=input_preprocessor.num_channels if input_preprocessor is not None else config.d_model\n",
    "        )\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(PERCEIVER_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    # @replace_return_docstrings(output_type=PerceiverModelOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        subsampled_output_points: Optional[Dict[str, torch.Tensor]] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, PerceiverModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import PerceiverConfig, PerceiverTokenizer, PerceiverImageProcessor, PerceiverModel\n",
    "        >>> from transformers.models.perceiver.modeling_perceiver import (\n",
    "        ...     PerceiverTextPreprocessor,\n",
    "        ...     PerceiverImagePreprocessor,\n",
    "        ...     PerceiverClassificationDecoder,\n",
    "        ... )\n",
    "        >>> import torch\n",
    "        >>> import requests\n",
    "        >>> from PIL import Image\n",
    "\n",
    "        >>> # EXAMPLE 1: using the Perceiver to classify texts\n",
    "        >>> # - we define a TextPreprocessor, which can be used to embed tokens\n",
    "        >>> # - we define a ClassificationDecoder, which can be used to decode the\n",
    "        >>> # final hidden states of the latents to classification logits\n",
    "        >>> # using trainable position embeddings\n",
    "        >>> config = PerceiverConfig()\n",
    "        >>> preprocessor = PerceiverTextPreprocessor(config)\n",
    "        >>> decoder = PerceiverClassificationDecoder(\n",
    "        ...     config,\n",
    "        ...     num_channels=config.d_latents,\n",
    "        ...     trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
    "        ...     use_query_residual=True,\n",
    "        ... )\n",
    "        >>> model = PerceiverModel(config, input_preprocessor=preprocessor, decoder=decoder)\n",
    "\n",
    "        >>> # you can then do a forward pass as follows:\n",
    "        >>> tokenizer = PerceiverTokenizer()\n",
    "        >>> text = \"hello world\"\n",
    "        >>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(inputs=inputs)\n",
    "        >>> logits = outputs.logits\n",
    "        >>> list(logits.shape)\n",
    "        [1, 2]\n",
    "\n",
    "        >>> # to train, one can train the model using standard cross-entropy:\n",
    "        >>> criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        >>> labels = torch.tensor([1])\n",
    "        >>> loss = criterion(logits, labels)\n",
    "\n",
    "        >>> # EXAMPLE 2: using the Perceiver to classify images\n",
    "        >>> # - we define an ImagePreprocessor, which can be used to embed images\n",
    "        >>> config = PerceiverConfig(image_size=224)\n",
    "        >>> preprocessor = PerceiverImagePreprocessor(\n",
    "        ...     config,\n",
    "        ...     prep_type=\"conv1x1\",\n",
    "        ...     spatial_downsample=1,\n",
    "        ...     out_channels=256,\n",
    "        ...     position_encoding_type=\"trainable\",\n",
    "        ...     concat_or_add_pos=\"concat\",\n",
    "        ...     project_pos_dim=256,\n",
    "        ...     trainable_position_encoding_kwargs=dict(\n",
    "        ...         num_channels=256,\n",
    "        ...         index_dims=config.image_size**2,\n",
    "        ...     ),\n",
    "        ... )\n",
    "\n",
    "        >>> model = PerceiverModel(\n",
    "        ...     config,\n",
    "        ...     input_preprocessor=preprocessor,\n",
    "        ...     decoder=PerceiverClassificationDecoder(\n",
    "        ...         config,\n",
    "        ...         num_channels=config.d_latents,\n",
    "        ...         trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
    "        ...         use_query_residual=True,\n",
    "        ...     ),\n",
    "        ... )\n",
    "\n",
    "        >>> # you can then do a forward pass as follows:\n",
    "        >>> image_processor = PerceiverImageProcessor()\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> inputs = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(inputs=inputs)\n",
    "        >>> logits = outputs.logits\n",
    "        >>> list(logits.shape)\n",
    "        [1, 2]\n",
    "\n",
    "        >>> # to train, one can train the model using standard cross-entropy:\n",
    "        >>> criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        >>> labels = torch.tensor([1])\n",
    "        >>> loss = criterion(logits, labels)\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.input_preprocessor is not None:\n",
    "            inputs, modality_sizes, inputs_without_pos = self.input_preprocessor(\n",
    "                inputs, interpolate_pos_encoding=interpolate_pos_encoding\n",
    "            )\n",
    "        else:\n",
    "            modality_sizes = None\n",
    "            inputs_without_pos = None\n",
    "            if inputs.size()[-1] != self.config.d_model:\n",
    "                raise ValueError(\n",
    "                    f\"Last dimension of the inputs: {inputs.size()[-1]} doesn't correspond to config.d_model:\"\n",
    "                    f\" {self.config.d_model}. Make sure to set config.d_model appropriately.\"\n",
    "                )\n",
    "\n",
    "        batch_size, seq_length, _ = inputs.size()\n",
    "        device = inputs.device\n",
    "\n",
    "        # If no attention mask is provided, make them all ones\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones((batch_size, seq_length), device=device)\n",
    "        # Make the attention mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        extended_attention_mask = self.invert_attention_mask(attention_mask)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_blocks x num_heads]\n",
    "        # and head_mask is converted to shape [num_blocks x batch x num_heads x N x N]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_blocks * self.config.num_self_attends_per_block)\n",
    "\n",
    "        embedding_output = self.embeddings(batch_size=batch_size)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=None,\n",
    "            head_mask=head_mask,\n",
    "            inputs=inputs,\n",
    "            inputs_mask=extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        logits = None\n",
    "        if self.decoder:\n",
    "            if subsampled_output_points is not None:\n",
    "                output_modality_sizes = {\n",
    "                    \"audio\": subsampled_output_points[\"audio\"].shape[0],\n",
    "                    \"image\": subsampled_output_points[\"image\"].shape[0],\n",
    "                    \"label\": 1,\n",
    "                }\n",
    "            else:\n",
    "                output_modality_sizes = modality_sizes\n",
    "            decoder_query = self.decoder.decoder_query(\n",
    "                inputs, modality_sizes, inputs_without_pos, subsampled_points=subsampled_output_points\n",
    "            )\n",
    "            decoder_outputs = self.decoder(\n",
    "                decoder_query,\n",
    "                z=sequence_output,\n",
    "                query_mask=extended_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            logits = decoder_outputs.logits\n",
    "\n",
    "            # add cross-attentions of decoder\n",
    "            if output_attentions and decoder_outputs.cross_attentions is not None:\n",
    "                if return_dict:\n",
    "                    encoder_outputs.cross_attentions = (\n",
    "                        encoder_outputs.cross_attentions + decoder_outputs.cross_attentions\n",
    "                    )\n",
    "                else:\n",
    "                    encoder_outputs = encoder_outputs + decoder_outputs.cross_attentions\n",
    "\n",
    "            if self.output_postprocessor:\n",
    "                logits = self.output_postprocessor(logits, modality_sizes=output_modality_sizes)\n",
    "\n",
    "        if not return_dict:\n",
    "            if logits is not None:\n",
    "                return (logits, sequence_output) + encoder_outputs[1:]\n",
    "            else:\n",
    "                return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return sequence_output\n",
    "        return PerceiverModelOutput(\n",
    "            logits=logits,\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "   \n",
    "class CustomPerceiverEmbeddings(PerceiverEmbeddings):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.custom_perceiver_embeddings_generator = CustomPerceiverEmbeddingGenerator(config=config)\n",
    "        self.latents = nn.Parameter(torch.randn(config.num_latents, config.d_latents))\n",
    "\n",
    "    def forward(self, batch_size): # hypotheses):\n",
    "        return self.latents.expand(batch_size, -1, -1)  # Thanks, Phil Wang\n",
    "        # return self.custom_perceiver_embeddings_generator(hypotheses)\n",
    "    \n",
    "    \n",
    "class CustomPerceiverModel(PerceiverModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        decoder=None,\n",
    "        input_preprocessor: PreprocessorType = None,\n",
    "        output_postprocessor: PostprocessorType = None,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.input_preprocessor = input_preprocessor\n",
    "        self.output_postprocessor = output_postprocessor\n",
    "        self.embeddings = CustomPerceiverEmbeddings(config)\n",
    "        self.encoder = PerceiverEncoder(\n",
    "            config, kv_dim=input_preprocessor.num_channels if input_preprocessor is not None else config.d_model\n",
    "        )\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(PERCEIVER_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "    # @replace_return_docstrings(output_type=PerceiverModelOutput, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.FloatTensor,\n",
    "        hypotheses: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        subsampled_output_points: Optional[Dict[str, torch.Tensor]] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, PerceiverModelOutput]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import PerceiverConfig, PerceiverTokenizer, PerceiverImageProcessor, PerceiverModel\n",
    "        >>> from transformers.models.perceiver.modeling_perceiver import (\n",
    "        ...     PerceiverTextPreprocessor,\n",
    "        ...     PerceiverImagePreprocessor,\n",
    "        ...     PerceiverClassificationDecoder,\n",
    "        ... )\n",
    "        >>> import torch\n",
    "        >>> import requests\n",
    "        >>> from PIL import Image\n",
    "\n",
    "        >>> # EXAMPLE 1: using the Perceiver to classify texts\n",
    "        >>> # - we define a TextPreprocessor, which can be used to embed tokens\n",
    "        >>> # - we define a ClassificationDecoder, which can be used to decode the\n",
    "        >>> # final hidden states of the latents to classification logits\n",
    "        >>> # using trainable position embeddings\n",
    "        >>> config = PerceiverConfig()\n",
    "        >>> preprocessor = PerceiverTextPreprocessor(config)\n",
    "        >>> decoder = PerceiverClassificationDecoder(\n",
    "        ...     config,\n",
    "        ...     num_channels=config.d_latents,\n",
    "        ...     trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
    "        ...     use_query_residual=True,\n",
    "        ... )\n",
    "        >>> model = PerceiverModel(config, input_preprocessor=preprocessor, decoder=decoder)\n",
    "\n",
    "        >>> # you can then do a forward pass as follows:\n",
    "        >>> tokenizer = PerceiverTokenizer()\n",
    "        >>> text = \"hello world\"\n",
    "        >>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(inputs=inputs)\n",
    "        >>> logits = outputs.logits\n",
    "        >>> list(logits.shape)\n",
    "        [1, 2]\n",
    "\n",
    "        >>> # to train, one can train the model using standard cross-entropy:\n",
    "        >>> criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        >>> labels = torch.tensor([1])\n",
    "        >>> loss = criterion(logits, labels)\n",
    "\n",
    "        >>> # EXAMPLE 2: using the Perceiver to classify images\n",
    "        >>> # - we define an ImagePreprocessor, which can be used to embed images\n",
    "        >>> config = PerceiverConfig(image_size=224)\n",
    "        >>> preprocessor = PerceiverImagePreprocessor(\n",
    "        ...     config,\n",
    "        ...     prep_type=\"conv1x1\",\n",
    "        ...     spatial_downsample=1,\n",
    "        ...     out_channels=256,\n",
    "        ...     position_encoding_type=\"trainable\",\n",
    "        ...     concat_or_add_pos=\"concat\",\n",
    "        ...     project_pos_dim=256,\n",
    "        ...     trainable_position_encoding_kwargs=dict(\n",
    "        ...         num_channels=256,\n",
    "        ...         index_dims=config.image_size**2,\n",
    "        ...     ),\n",
    "        ... )\n",
    "\n",
    "        >>> model = PerceiverModel(\n",
    "        ...     config,\n",
    "        ...     input_preprocessor=preprocessor,\n",
    "        ...     decoder=PerceiverClassificationDecoder(\n",
    "        ...         config,\n",
    "        ...         num_channels=config.d_latents,\n",
    "        ...         trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n",
    "        ...         use_query_residual=True,\n",
    "        ...     ),\n",
    "        ... )\n",
    "\n",
    "        >>> # you can then do a forward pass as follows:\n",
    "        >>> image_processor = PerceiverImageProcessor()\n",
    "        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "        >>> inputs = image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     outputs = model(inputs=inputs)\n",
    "        >>> logits = outputs.logits\n",
    "        >>> list(logits.shape)\n",
    "        [1, 2]\n",
    "\n",
    "        >>> # to train, one can train the model using standard cross-entropy:\n",
    "        >>> criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        >>> labels = torch.tensor([1])\n",
    "        >>> loss = criterion(logits, labels)\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.input_preprocessor is not None:\n",
    "            inputs, modality_sizes, inputs_without_pos = self.input_preprocessor(\n",
    "                inputs, interpolate_pos_encoding=interpolate_pos_encoding\n",
    "            )\n",
    "        else:\n",
    "            modality_sizes = None\n",
    "            inputs_without_pos = None\n",
    "            if inputs.size()[-1] != self.config.d_model:\n",
    "                raise ValueError(\n",
    "                    f\"Last dimension of the inputs: {inputs.size()[-1]} doesn't correspond to config.d_model:\"\n",
    "                    f\" {self.config.d_model}. Make sure to set config.d_model appropriately.\"\n",
    "                )\n",
    "\n",
    "        batch_size, seq_length, _ = inputs.size()\n",
    "        device = inputs.device\n",
    "\n",
    "        # If no attention mask is provided, make them all ones\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones((batch_size, seq_length), device=device)\n",
    "        # Make the attention mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        extended_attention_mask = self.invert_attention_mask(attention_mask)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_blocks x num_heads]\n",
    "        # and head_mask is converted to shape [num_blocks x batch x num_heads x N x N]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_blocks * self.config.num_self_attends_per_block)\n",
    "\n",
    "        # embedding_output = self.embeddings(hypotheses=hypotheses)\n",
    "        embedding_output = self.embeddings(batch_size=batch_size)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=None,\n",
    "            head_mask=head_mask,\n",
    "            inputs=inputs,\n",
    "            inputs_mask=extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        logits = None\n",
    "        if self.decoder:\n",
    "            if subsampled_output_points is not None:\n",
    "                output_modality_sizes = {\n",
    "                    \"audio\": subsampled_output_points[\"audio\"].shape[0],\n",
    "                    \"image\": subsampled_output_points[\"image\"].shape[0],\n",
    "                    \"label\": 1,\n",
    "                }\n",
    "            else:\n",
    "                output_modality_sizes = modality_sizes\n",
    "            decoder_query = self.decoder.decoder_query(\n",
    "                inputs, modality_sizes, inputs_without_pos, subsampled_points=subsampled_output_points\n",
    "            )\n",
    "            decoder_outputs = self.decoder(\n",
    "                decoder_query,\n",
    "                z=sequence_output,\n",
    "                query_mask=extended_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            logits = decoder_outputs.logits\n",
    "\n",
    "            # add cross-attentions of decoder\n",
    "            if output_attentions and decoder_outputs.cross_attentions is not None:\n",
    "                if return_dict:\n",
    "                    encoder_outputs.cross_attentions = (\n",
    "                        encoder_outputs.cross_attentions + decoder_outputs.cross_attentions\n",
    "                    )\n",
    "                else:\n",
    "                    encoder_outputs = encoder_outputs + decoder_outputs.cross_attentions\n",
    "\n",
    "            if self.output_postprocessor:\n",
    "                logits = self.output_postprocessor(logits, modality_sizes=output_modality_sizes)\n",
    "\n",
    "        if not return_dict:\n",
    "            if logits is not None:\n",
    "                return (logits, sequence_output) + encoder_outputs[1:]\n",
    "            else:\n",
    "                return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return PerceiverModelOutput(\n",
    "            logits=logits,\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class CustomPerceiverForContradictionDetection(PerceiverForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        trainable_position_encoding_kwargs_decoder = {\"num_channels\": config.d_latents, \"index_dims\": 1}\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.perceiver = CustomPerceiverModel(\n",
    "            config,\n",
    "            input_preprocessor=PerceiverTextPreprocessor(config),\n",
    "            decoder=PerceiverClassificationDecoder(\n",
    "                config,\n",
    "                num_channels=config.d_latents,\n",
    "                trainable_position_encoding_kwargs=trainable_position_encoding_kwargs_decoder,\n",
    "                use_query_residual=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        hypotheses = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        hypothesis_ids: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple, PerceiverClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the classification/regression loss. Indices should be in `[0, ..., config.num_labels -\n",
    "            1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If `config.num_labels >\n",
    "            1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, PerceiverForSequenceClassification\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n",
    "        >>> model = PerceiverForSequenceClassification.from_pretrained(\"deepmind/language-perceiver\")\n",
    "\n",
    "        >>> text = \"hello world\"\n",
    "        >>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "        >>> outputs = model(inputs=inputs)\n",
    "        >>> logits = outputs.logits\n",
    "        >>> list(logits.shape)\n",
    "        [1, 2]\n",
    "        ```\"\"\"\n",
    "        hypotheses = hypothesis_ids\n",
    "        if inputs is not None and input_ids is not None:\n",
    "            raise ValueError(\"You cannot use both `inputs` and `input_ids`\")\n",
    "        elif inputs is None and input_ids is not None:\n",
    "            inputs = input_ids\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.perceiver(\n",
    "            inputs=inputs,\n",
    "            hypotheses=hypotheses,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits if return_dict else outputs[0]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return PerceiverClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_model = CustomPerceiverForContradictionDetection(config=PerceiverConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_model(torch.tensor([[1, 2, 3, 4, 5]]), torch.tensor([[6, 7, 8, 9, 10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json\n",
    "from transformers import PerceiverTokenizer, PerceiverModel, PerceiverConfig, PerceiverPreTrainedModel, PerceiverForSequenceClassification, TrainingArguments, Trainer, \\\n",
    "    DataCollatorWithPadding\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "ROOT_PATH = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get ContractNLI dataset\n",
    "\n",
    "id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMnetioned\"}\n",
    "label2id = {\"Entailment\": 0, \"Contradiction\": 1, \"NotMentioned\": 2}\n",
    "\n",
    "def load_dataset_custom(dataset_name):\n",
    "    if dataset_name == \"contract-nli\":\n",
    "        def contract_nli_iterator(data):\n",
    "            documents, labels = data['documents'], data['labels']\n",
    "            for document in documents:\n",
    "                id = document['id']\n",
    "                file_name = document['file_name']\n",
    "                text = document['text']\n",
    "                spans = document['spans']\n",
    "                annotation_sets = document['annotation_sets']\n",
    "                document_type = document['document_type']\n",
    "                url = document['url']\n",
    "                for annotation_id, annotation_content in annotation_sets[0]['annotations'].items():\n",
    "                    hypothesis = labels[annotation_id]['hypothesis']\n",
    "                    choice = annotation_content['choice']\n",
    "                    yield {\n",
    "                        \"id\": id,\n",
    "                        \"file_name\": file_name,\n",
    "                        \"text\": text,\n",
    "                        \"spans\": spans,\n",
    "                        \"document_type\": document_type,\n",
    "                        \"url\": url,\n",
    "                        \"hypotheses\": hypothesis,\n",
    "                        \"labels\": label2id[choice],\n",
    "                    }            \n",
    "        base_filepath = os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli\")\n",
    "        train_filepath = os.path.join(base_filepath, \"train.json\")\n",
    "        validation_filepath = os.path.join(base_filepath, \"dev.json\")\n",
    "        test_filepath = os.path.join(base_filepath, \"test.json\")\n",
    "        with open(train_filepath) as f:\n",
    "            train_data = json.load(f)\n",
    "        with open(validation_filepath) as f:\n",
    "            validation_data = json.load(f)\n",
    "        with open(test_filepath) as f:\n",
    "            test_data = json.load(f)\n",
    "        data = {\n",
    "            \"train\": Dataset.from_generator(lambda: contract_nli_iterator(train_data)),\n",
    "            \"validation\": Dataset.from_generator(lambda: contract_nli_iterator(validation_data)),\n",
    "            \"test\": Dataset.from_generator(lambda: contract_nli_iterator(test_data)),\n",
    "        }\n",
    "        return DatasetDict(data)\n",
    "    return None\n",
    "\n",
    "contract_nli_dataset = load_dataset_custom(\"contract-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max premise length is 54571\n",
      "max hypothesis length is 162\n"
     ]
    }
   ],
   "source": [
    "# find out the max length of hypothesis and premise across train, validation, and test\n",
    "# and pad to these length to prevent trainer complaining\n",
    "\n",
    "premise_lengths = []\n",
    "hypothesis_lengths = []\n",
    "for mode in ['train', 'validation', 'test']:\n",
    "    contract_nli_dataset_mode = contract_nli_dataset[mode]\n",
    "    for e in contract_nli_dataset_mode:\n",
    "        premise = e['text']\n",
    "        hypothesis = e['hypotheses']\n",
    "        premise_lengths.append(len(premise))\n",
    "        hypothesis_lengths.append(len(hypothesis))\n",
    "print(f\"max premise length is {max(premise_lengths)}\")\n",
    "print(f\"max hypothesis length is {max(hypothesis_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_premise_length = 54571\n",
    "max_hypothesis_length = 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get a proper model\n",
    "\n",
    "model_max_length = 60000\n",
    "hypothesis_max_length = 200\n",
    "perceiver_tokenizer = PerceiverTokenizer(model_max_length=model_max_length)\n",
    "perceiver_tokenizer_small = PerceiverTokenizer(model_max_length=hypothesis_max_length)\n",
    "\n",
    "def process_contract_nli_dataset(element):\n",
    "    tmp1 = perceiver_tokenizer(element['text'], truncation=True, padding=\"max_length\")\n",
    "    tmp2 = perceiver_tokenizer_small(element['hypotheses'], truncation=True, padding=\"max_length\")\n",
    "    ans = {\n",
    "        'input_ids': tmp1['input_ids'],\n",
    "        'hypothesis_ids': tmp2['input_ids']\n",
    "    }\n",
    "    return ans\n",
    "contract_nli_dataset_processed = contract_nli_dataset.map(process_contract_nli_dataset, batched=True)\n",
    "contract_nli_dataset_processed = contract_nli_dataset_processed.remove_columns(['file_name', 'text', 'document_type', 'url', 'hypotheses'])\n",
    "contract_nli_dataset_processed = contract_nli_dataset_processed.rename_column(\"hypothesis_ids\", \"hypotheses\")\n",
    "\n",
    "# contract_nli_dataset_processed = contract_nli_dataset\n",
    "custom_perceiver_config = PerceiverConfig(model_max_length=model_max_length, num_labels=3, max_position_embeddings=model_max_length + 1000)\n",
    "custom_perceiver_model = CustomPerceiverForContradictionDetection(config=custom_perceiver_config)\n",
    "perceiver_data_collator = DataCollatorWithPadding(tokenizer=perceiver_tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'spans', 'labels', 'input_ids', 'hypotheses'])\n",
      "dict_keys(['id', 'file_name', 'text', 'spans', 'document_type', 'url', 'hypotheses', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(contract_nli_dataset_processed['train'][0].keys())\n",
    "print(contract_nli_dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(contract_nli_dataset_processed['train'][1]['hypotheses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ../ignored_dir/training_outputs/custom_perceiver_contract_nli/run_48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "## training setup\n",
    "\n",
    "ignored_dir_path = os.path.join(ROOT_PATH, \"ignored_dir\")\n",
    "if not os.path.exists(ignored_dir_path):\n",
    "    os.mkdir(ignored_dir_path)\n",
    "training_outputs_path = os.path.join(ignored_dir_path, \"training_outputs\")\n",
    "if not os.path.exists(training_outputs_path):\n",
    "    os.mkdir(training_outputs_path)\n",
    "output_path = os.path.join(training_outputs_path, \"custom_perceiver_contract_nli\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "output_path_content = os.listdir(output_path)\n",
    "pattern = \"^run_([0-9]+)\"\n",
    "output_path_legal_content = [e for e in output_path_content if re.match(pattern, e)]\n",
    "run_output_path = os.path.join(output_path, f\"run_{len(output_path_legal_content) + 1}\")\n",
    "print(f\"saving to {run_output_path}\")\n",
    "perceiver_training_arguments = TrainingArguments(\n",
    "    run_output_path,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate = 1e-4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def perceiver_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)    \n",
    "\n",
    "perceiver_trainer = Trainer(\n",
    "    model=custom_perceiver_model,\n",
    "    args=perceiver_training_arguments,\n",
    "    train_dataset=contract_nli_dataset_processed['train'],\n",
    "    eval_dataset=contract_nli_dataset_processed[\"validation\"],\n",
    "    tokenizer=perceiver_tokenizer,\n",
    "    data_collator=perceiver_data_collator,\n",
    "    compute_metrics=perceiver_compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='4480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 451/4480 54:05 < 8:05:23, 0.14 it/s, Epoch 2.00/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.941781</td>\n",
       "      <td>0.469624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.964300</td>\n",
       "      <td>0.921178</td>\n",
       "      <td>0.528447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperceiver_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2237\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2870\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2870\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2871\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/arrow_dataset.py:2851\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2849\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2850\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2851\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:449\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 449\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/scc_yan/virtual-env/lib/python3.10/site-packages/datasets/formatting/formatting.py:151\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perceiver_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
