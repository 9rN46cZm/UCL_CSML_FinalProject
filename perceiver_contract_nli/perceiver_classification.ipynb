{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json\n",
    "from transformers import PerceiverTokenizer, PerceiverModel, PerceiverConfig, PerceiverPreTrainedModel, PerceiverForSequenceClassification, TrainingArguments, Trainer, \\\n",
    "    DataCollatorWithPadding\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli/train.json\")) as train_json_f:\n",
    "    train_json = json.load(train_json_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_json): <class 'dict'>\n",
      "train_json.keys(): dict_keys(['documents', 'labels'])\n",
      "type(train_json['documents']): <class 'list'>\n",
      "type(train_json['labels']): <class 'dict'>\n",
      "len(train_json['documents']): 423\n",
      "train_json['documents'][0].keys(): dict_keys(['id', 'file_name', 'text', 'spans', 'annotation_sets', 'document_type', 'url'])\n",
      "type(train_json['documents'][0]['annotation_sets']): <class 'list'>\n",
      "len(train_json['documents'][0]['annotation_sets']): 1\n",
      "type(train_json['documents'][0]['annotation_sets][0]): <class 'dict'>\n",
      "train_json['documents'][0]['annotation_sets'][0].keys(): dict_keys(['annotations'])\n",
      "type(train_json['documents'][0]['annotation_sets'][0]['annotations']): dict_keys(['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4'])\n",
      "type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']): <class 'dict'>\n",
      "train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11'].keys(): dict_keys(['choice', 'spans'])\n",
      "type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['choice']): <class 'str'>\n",
      "type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans']): <class 'list'>\n",
      "len(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans']): 0\n",
      "train_json['labels'].keys(): dict_keys(['nda-11', 'nda-16', 'nda-15', 'nda-10', 'nda-2', 'nda-1', 'nda-19', 'nda-12', 'nda-20', 'nda-3', 'nda-18', 'nda-7', 'nda-17', 'nda-8', 'nda-13', 'nda-5', 'nda-4'])\n",
      "type(train_json['labels']['nda-11']): <class 'dict'>\n",
      "train_json['labels']['nda-11'].keys(): dict_keys(['short_description', 'hypothesis'])\n",
      "type(train_json['labels']['nda-11']['short_description']): <class 'str'>\n",
      "type(train_json['labels']['nda-11']['hypothesis']): <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(train_json): {type(train_json)}\")\n",
    "print(f\"train_json.keys(): {train_json.keys()}\")\n",
    "print(f\"type(train_json['documents']): {type(train_json['documents'])}\")\n",
    "print(f\"type(train_json['labels']): {type(train_json['labels'])}\")\n",
    "print(f\"len(train_json['documents']): {len(train_json['documents'])}\")\n",
    "print(f\"train_json['documents'][0].keys(): {train_json['documents'][0].keys()}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets']): {type(train_json['documents'][0]['annotation_sets'])}\")\n",
    "print(f\"len(train_json['documents'][0]['annotation_sets']): {len(train_json['documents'][0]['annotation_sets'])}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets][0]): {type(train_json['documents'][0]['annotation_sets'][0])}\")\n",
    "print(f\"train_json['documents'][0]['annotation_sets'][0].keys(): {train_json['documents'][0]['annotation_sets'][0].keys()}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets'][0]['annotations']): {train_json['documents'][0]['annotation_sets'][0]['annotations'].keys()}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']): {type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11'])}\")\n",
    "print(f\"train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11'].keys(): {train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11'].keys()}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['choice']): {type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['choice'])}\")\n",
    "print(f\"type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans']): {type(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans'])}\")\n",
    "print(f\"len(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans']): {len(train_json['documents'][0]['annotation_sets'][0]['annotations']['nda-11']['spans'])}\")\n",
    "print(f\"train_json['labels'].keys(): {train_json['labels'].keys()}\")\n",
    "print(f\"type(train_json['labels']['nda-11']): {type(train_json['labels']['nda-11'])}\")\n",
    "print(f\"train_json['labels']['nda-11'].keys(): {train_json['labels']['nda-11'].keys()}\")\n",
    "print(f\"type(train_json['labels']['nda-11']['short_description']): {type(train_json['labels']['nda-11']['short_description'])}\")\n",
    "print(f\"type(train_json['labels']['nda-11']['hypothesis']): {type(train_json['labels']['nda-11']['hypothesis'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Entailment\", 1: \"Contradiction\", 2: \"NotMnetioned\"}\n",
    "label2id = {\"Entailment\": 0, \"Contradiction\": 1, \"NotMentioned\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_custom(dataset_name):\n",
    "    if dataset_name == \"contract-nli\":\n",
    "        def contract_nli_iterator(data):\n",
    "            documents, labels = data['documents'], data['labels']\n",
    "            for document in documents:\n",
    "                id = document['id']\n",
    "                file_name = document['file_name']\n",
    "                text = document['text']\n",
    "                spans = document['spans']\n",
    "                annotation_sets = document['annotation_sets']\n",
    "                document_type = document['document_type']\n",
    "                url = document['url']\n",
    "                for annotation_id, annotation_content in annotation_sets[0]['annotations'].items():\n",
    "                    hypothesis = labels[annotation_id]['hypothesis']\n",
    "                    choice = annotation_content['choice']\n",
    "                    yield {\n",
    "                        \"id\": id,\n",
    "                        \"file_name\": file_name,\n",
    "                        \"text\": text,\n",
    "                        \"spans\": spans,\n",
    "                        \"document_type\": document_type,\n",
    "                        \"url\": url,\n",
    "                        \"hypothesis\": hypothesis,\n",
    "                        \"labels\": label2id[choice],\n",
    "                    }            \n",
    "        base_filepath = os.path.join(ROOT_PATH, \"ignored_dir/data/contract-nli\")\n",
    "        train_filepath = os.path.join(base_filepath, \"train.json\")\n",
    "        validation_filepath = os.path.join(base_filepath, \"dev.json\")\n",
    "        test_filepath = os.path.join(base_filepath, \"test.json\")\n",
    "        with open(train_filepath) as f:\n",
    "            train_data = json.load(f)\n",
    "        with open(validation_filepath) as f:\n",
    "            validation_data = json.load(f)\n",
    "        with open(test_filepath) as f:\n",
    "            test_data = json.load(f)\n",
    "        data = {\n",
    "            \"train\": Dataset.from_generator(lambda: contract_nli_iterator(train_data)),\n",
    "            \"validation\": Dataset.from_generator(lambda: contract_nli_iterator(validation_data)),\n",
    "            \"test\": Dataset.from_generator(lambda: contract_nli_iterator(test_data)),\n",
    "        }\n",
    "        return DatasetDict(data)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_nli_dataset = load_dataset_custom(\"contract-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = 60000 # set a big number here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceiver_tokenizer = PerceiverTokenizer(model_max_length=model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_contract_nli_dataset(element):\n",
    "    return perceiver_tokenizer(element['text'], element['hypothesis'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_nli_dataset_processed = contract_nli_dataset.map(process_contract_nli_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'file_name', 'text', 'spans', 'document_type', 'url', 'hypothesis', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 7191\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'file_name', 'text', 'spans', 'document_type', 'url', 'hypothesis', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1037\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'file_name', 'text', 'spans', 'document_type', 'url', 'hypothesis', 'labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2091\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(contract_nli_dataset_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# for loop to figure out how long the tokenized sentences are\\nrecord = []\\nfor mode in [\\'train\\', \\'validation\\', \\'test\\']:\\n    mode_dataset = contract_nli_dataset_processed[mode]\\n    for i in tqdm(range(len(mode_dataset)), total=len(mode_dataset)):\\n        record.append(len(mode_dataset[i][\\'input_ids\\']))\\nprint(f\"max length of tokenized dataset element is: {max(record)}\") # 55122\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# for loop to figure out how long the tokenized sentences are\n",
    "record = []\n",
    "for mode in ['train', 'validation', 'test']:\n",
    "    mode_dataset = contract_nli_dataset_processed[mode]\n",
    "    for i in tqdm(range(len(mode_dataset)), total=len(mode_dataset)):\n",
    "        record.append(len(mode_dataset[i]['input_ids']))\n",
    "print(f\"max length of tokenized dataset element is: {max(record)}\") # 55122\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceiver_config = PerceiverConfig(model_max_length=model_max_length, num_labels=3, max_position_embeddings=model_max_length + 1000, num_self_attends_per_block=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PerceiverForSequenceClassification were not initialized from the model checkpoint at deepmind/language-perceiver and are newly initialized: ['perceiver.decoder.decoder.decoding_cross_attention.attention.output.dense.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.output.dense.weight', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.key.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.key.weight', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.layernorm1.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.layernorm1.weight', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.layernorm2.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.layernorm2.weight', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.query.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.query.weight', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.value.bias', 'perceiver.decoder.decoder.decoding_cross_attention.attention.self.value.weight', 'perceiver.decoder.decoder.decoding_cross_attention.layernorm.bias', 'perceiver.decoder.decoder.decoding_cross_attention.layernorm.weight', 'perceiver.decoder.decoder.decoding_cross_attention.mlp.dense1.bias', 'perceiver.decoder.decoder.decoding_cross_attention.mlp.dense1.weight', 'perceiver.decoder.decoder.decoding_cross_attention.mlp.dense2.bias', 'perceiver.decoder.decoder.decoding_cross_attention.mlp.dense2.weight', 'perceiver.decoder.decoder.final_layer.bias', 'perceiver.decoder.decoder.final_layer.weight', 'perceiver.decoder.decoder.output_position_encodings.position_embeddings']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of PerceiverForSequenceClassification were not initialized from the model checkpoint at deepmind/language-perceiver and are newly initialized because the shapes did not match:\n",
      "- perceiver.encoder.cross_attention.attention.output.dense.weight: found shape torch.Size([1280, 1280]) in the checkpoint and torch.Size([1280, 768]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.key.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.key.weight: found shape torch.Size([256, 768]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.query.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.query.weight: found shape torch.Size([256, 1280]) in the checkpoint and torch.Size([768, 1280]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.value.bias: found shape torch.Size([1280]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- perceiver.encoder.cross_attention.attention.self.value.weight: found shape torch.Size([1280, 768]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.0.attention.self.key.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.0.attention.self.key.weight: found shape torch.Size([256, 1280]) in the checkpoint and torch.Size([1280, 1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.0.attention.self.query.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.0.attention.self.query.weight: found shape torch.Size([256, 1280]) in the checkpoint and torch.Size([1280, 1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.1.attention.self.key.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.1.attention.self.key.weight: found shape torch.Size([256, 1280]) in the checkpoint and torch.Size([1280, 1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.1.attention.self.query.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([1280]) in the model instantiated\n",
      "- perceiver.encoder.self_attends.1.attention.self.query.weight: found shape torch.Size([256, 1280]) in the checkpoint and torch.Size([1280, 1280]) in the model instantiated\n",
      "- perceiver.input_preprocessor.position_embeddings.weight: found shape torch.Size([2048, 768]) in the checkpoint and torch.Size([61000, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = os.path.join(ROOT_PATH, \"ignored_dir/training_outputs/perceiver_contract_nli/run_5/checkpoint-309\")\n",
    "# perceiver_model = PerceiverForSequenceClassification.from_pretrained(checkpoint_path, config=perceiver_config, ignore_mismatched_sizes=True)\n",
    "# perceiver_model = PerceiverForSequenceClassification(config=perceiver_config)\n",
    "perceiver_model = PerceiverForSequenceClassification.from_pretrained(\"deepmind/language-perceiver\", config=perceiver_config, offload_state_dict=True, torch_dtype=torch.float16, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceiver_data_collator = DataCollatorWithPadding(tokenizer=perceiver_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ../ignored_dir/training_outputs/perceiver_contract_nli/run_6\n"
     ]
    }
   ],
   "source": [
    "ignored_dir_path = os.path.join(ROOT_PATH, \"ignored_dir\")\n",
    "if not os.path.exists(ignored_dir_path):\n",
    "    os.mkdir(ignored_dir_path)\n",
    "training_outputs_path = os.path.join(ignored_dir_path, \"training_outputs\")\n",
    "if not os.path.exists(training_outputs_path):\n",
    "    os.mkdir(training_outputs_path)\n",
    "output_path = os.path.join(training_outputs_path, \"perceiver_contract_nli\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "output_path_content = os.listdir(output_path)\n",
    "pattern = \"^run_([0-9]+)\"\n",
    "output_path_legal_content = [e for e in output_path_content if re.match(pattern, e)]\n",
    "run_output_path = os.path.join(output_path, f\"run_{len(output_path_legal_content) + 1}\")\n",
    "print(f\"saving to {run_output_path}\")\n",
    "perceiver_training_arguments = TrainingArguments(\n",
    "    run_output_path,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate = 1e-6,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceiver_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "perceiver_trainer = Trainer(\n",
    "    model=perceiver_model,\n",
    "    args=perceiver_training_arguments,\n",
    "    train_dataset=contract_nli_dataset_processed['train'],\n",
    "    eval_dataset=contract_nli_dataset_processed[\"validation\"],\n",
    "    tokenizer=perceiver_tokenizer,\n",
    "    data_collator=perceiver_data_collator,\n",
    "   #  compute_metrics=perceiver_compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret = perceiver_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/yan_xu_uk_qbe_com/scc_yan/virtual-env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2544, -0.0618, -0.6348],\n",
      "        [ 0.2549, -0.0594, -0.6353]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3027,  0.4028, -0.6045,  ...,  2.5684,  1.4375, -1.0410],\n",
      "         [ 0.6514,  1.4385, -0.7861,  ...,  0.5073, -2.1895,  0.8115],\n",
      "         [ 1.8955, -0.3945,  0.0125,  ...,  0.1781,  0.7720,  0.2490],\n",
      "         ...,\n",
      "         [ 0.8218, -0.1284,  0.4758,  ...,  0.0783,  0.0542, -0.5781],\n",
      "         [ 0.8018, -0.5693, -0.2341,  ...,  0.7026, -0.0247,  0.3159],\n",
      "         [ 2.4434, -0.0592, -0.1635,  ...,  0.3896,  0.3745, -0.1616]],\n",
      "\n",
      "        [[ 1.3096,  0.3799, -0.6372,  ...,  2.5898,  1.4375, -1.0459],\n",
      "         [ 0.6768,  1.3838, -0.7490,  ...,  0.4561, -2.1387,  0.8149],\n",
      "         [ 1.8711, -0.4089,  0.0217,  ...,  0.1956,  0.7988,  0.2097],\n",
      "         ...,\n",
      "         [ 0.8345, -0.1356,  0.5376,  ...,  0.0743,  0.1079, -0.6328],\n",
      "         [ 0.7803, -0.5854, -0.2334,  ...,  0.6885, -0.0625,  0.3259],\n",
      "         [ 2.3750, -0.0555, -0.1641,  ...,  0.3618,  0.4031, -0.2166]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PereiverFSC forward(): set to single_label_classification\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2544, -0.0616, -0.6333],\n",
      "        [ 0.2556, -0.0592, -0.6348]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3164,  0.3850, -0.6260,  ...,  2.5469,  1.3867, -1.0166],\n",
      "         [ 0.6772,  1.3848, -0.7227,  ...,  0.4922, -2.1797,  0.8291],\n",
      "         [ 1.9258, -0.3599,  0.0180,  ...,  0.1793,  0.7964,  0.2379],\n",
      "         ...,\n",
      "         [ 0.7983, -0.1382,  0.5405,  ...,  0.1199,  0.0327, -0.5869],\n",
      "         [ 0.8408, -0.5713, -0.2001,  ...,  0.7026, -0.0569,  0.3467],\n",
      "         [ 2.5098, -0.0558, -0.1316,  ...,  0.3970,  0.3235, -0.1685]],\n",
      "\n",
      "        [[ 1.3115,  0.3789, -0.6465,  ...,  2.5898,  1.4248, -1.0391],\n",
      "         [ 0.6807,  1.3828, -0.7373,  ...,  0.4548, -2.1426,  0.8193],\n",
      "         [ 1.8691, -0.4031,  0.0201,  ...,  0.1953,  0.8008,  0.2095],\n",
      "         ...,\n",
      "         [ 0.8262, -0.1379,  0.5508,  ...,  0.0798,  0.1064, -0.6299],\n",
      "         [ 0.7812, -0.5859, -0.2267,  ...,  0.6929, -0.0701,  0.3367],\n",
      "         [ 2.3730, -0.0560, -0.1616,  ...,  0.3687,  0.3933, -0.2144]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2544, -0.0616, -0.6333],\n",
      "        [ 0.2556, -0.0592, -0.6348]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives 1.650390625\n",
      "loss = loss_fct(tensor([[ 0.2544, -0.0618, -0.6348],\n",
      "        [ 0.2549, -0.0594, -0.6353]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives 1.2060546875\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2568, -0.0623, -0.6348],\n",
      "        [ 0.2551, -0.0566, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.2988e+00,  3.9697e-01, -5.8691e-01,  ...,  2.5645e+00,\n",
      "           1.4443e+00, -1.0459e+00],\n",
      "         [ 7.0850e-01,  1.4922e+00, -7.4561e-01,  ...,  5.1953e-01,\n",
      "          -2.2168e+00,  7.8662e-01],\n",
      "         [ 1.8418e+00, -4.0234e-01,  6.1035e-04,  ...,  2.3218e-01,\n",
      "           8.2324e-01,  2.0239e-01],\n",
      "         ...,\n",
      "         [ 7.9395e-01, -1.6333e-01,  5.2002e-01,  ...,  9.7229e-02,\n",
      "           1.1719e-01, -5.8789e-01],\n",
      "         [ 7.5977e-01, -5.4199e-01, -2.1594e-01,  ...,  7.3535e-01,\n",
      "          -1.2695e-02,  3.3154e-01],\n",
      "         [ 2.4004e+00, -9.5825e-02, -1.6602e-01,  ...,  3.7500e-01,\n",
      "           3.5913e-01, -1.8018e-01]],\n",
      "\n",
      "        [[ 1.3330e+00,  3.6694e-01, -6.2402e-01,  ...,  2.5664e+00,\n",
      "           1.4395e+00, -1.0439e+00],\n",
      "         [ 6.8506e-01,  1.4355e+00, -7.7246e-01,  ...,  4.9805e-01,\n",
      "          -2.1953e+00,  7.5879e-01],\n",
      "         [ 1.8701e+00, -4.0381e-01,  4.1931e-02,  ...,  1.7932e-01,\n",
      "           7.9102e-01,  2.2620e-01],\n",
      "         ...,\n",
      "         [ 8.2812e-01, -1.1719e-01,  5.2832e-01,  ...,  9.6375e-02,\n",
      "           9.8145e-02, -6.5430e-01],\n",
      "         [ 7.8223e-01, -5.3613e-01, -2.4988e-01,  ...,  6.5967e-01,\n",
      "          -5.2490e-02,  3.5278e-01],\n",
      "         [ 2.3672e+00, -6.5308e-02, -1.5796e-01,  ...,  3.8696e-01,\n",
      "           3.5938e-01, -2.1252e-01]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2561, -0.0615, -0.6343],\n",
      "        [ 0.2549, -0.0565, -0.6362]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3057,  0.3896, -0.5947,  ...,  2.5684,  1.4229, -1.0361],\n",
      "         [ 0.7261,  1.4707, -0.7383,  ...,  0.5034, -2.2148,  0.7891],\n",
      "         [ 1.8633, -0.3882,  0.0188,  ...,  0.2332,  0.8423,  0.1976],\n",
      "         ...,\n",
      "         [ 0.8027, -0.1788,  0.5273,  ...,  0.1188,  0.1064, -0.5996],\n",
      "         [ 0.7783, -0.5464, -0.2046,  ...,  0.7373, -0.0271,  0.3381],\n",
      "         [ 2.4375, -0.0985, -0.1469,  ...,  0.3733,  0.3413, -0.1844]],\n",
      "\n",
      "        [[ 1.3311,  0.3694, -0.6240,  ...,  2.5703,  1.4453, -1.0439],\n",
      "         [ 0.6831,  1.4346, -0.7803,  ...,  0.4976, -2.1934,  0.7637],\n",
      "         [ 1.8682, -0.4043,  0.0393,  ...,  0.1807,  0.7930,  0.2289],\n",
      "         ...,\n",
      "         [ 0.8242, -0.1171,  0.5200,  ...,  0.0970,  0.1011, -0.6553],\n",
      "         [ 0.7803, -0.5352, -0.2539,  ...,  0.6602, -0.0520,  0.3523],\n",
      "         [ 2.3574, -0.0692, -0.1560,  ...,  0.3899,  0.3608, -0.2162]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2568, -0.0623, -0.6348],\n",
      "        [ 0.2551, -0.0566, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives 0.7607421875\n",
      "loss = loss_fct(tensor([[ 0.2561, -0.0615, -0.6343],\n",
      "        [ 0.2549, -0.0565, -0.6362]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives 1.2060546875\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2595, -0.0715, -0.6475],\n",
      "        [ 0.2566, -0.0615, -0.6367]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3350,  0.3569, -0.6460,  ...,  2.5625,  1.4170, -1.0547],\n",
      "         [ 0.7256,  1.3555, -0.7715,  ...,  0.5337, -2.2344,  0.7422],\n",
      "         [ 1.8867, -0.4194,  0.0077,  ...,  0.2219,  0.8125,  0.2155],\n",
      "         ...,\n",
      "         [ 0.7734, -0.1163,  0.5181,  ...,  0.1049,  0.1104, -0.5273],\n",
      "         [ 0.8091, -0.5986, -0.2485,  ...,  0.7109, -0.0759,  0.3518],\n",
      "         [ 2.4570, -0.0712, -0.1735,  ...,  0.3845,  0.3865, -0.2021]],\n",
      "\n",
      "        [[ 1.3281,  0.3955, -0.6309,  ...,  2.5625,  1.4170, -1.0371],\n",
      "         [ 0.7197,  1.3896, -0.7622,  ...,  0.5117, -2.2266,  0.7969],\n",
      "         [ 1.8730, -0.3667,  0.0337,  ...,  0.1816,  0.8018,  0.2329],\n",
      "         ...,\n",
      "         [ 0.8096, -0.1227,  0.4788,  ...,  0.1215,  0.0737, -0.5820],\n",
      "         [ 0.7915, -0.5933, -0.2676,  ...,  0.7324, -0.0793,  0.3281],\n",
      "         [ 2.4844, -0.0349, -0.1567,  ...,  0.3994,  0.3796, -0.1576]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2585, -0.0705, -0.6479],\n",
      "        [ 0.2563, -0.0613, -0.6362]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3252,  0.3643, -0.6411,  ...,  2.5586,  1.4463, -1.0537],\n",
      "         [ 0.7109,  1.3604, -0.7734,  ...,  0.5083, -2.2148,  0.7383],\n",
      "         [ 1.8623, -0.4316,  0.0027,  ...,  0.2190,  0.7998,  0.2239],\n",
      "         ...,\n",
      "         [ 0.7808, -0.0982,  0.4897,  ...,  0.0853,  0.1125, -0.5371],\n",
      "         [ 0.7910, -0.5869, -0.2590,  ...,  0.7109, -0.0681,  0.3469],\n",
      "         [ 2.4004, -0.0756, -0.1680,  ...,  0.3745,  0.3982, -0.2086]],\n",
      "\n",
      "        [[ 1.3252,  0.3955, -0.6294,  ...,  2.5723,  1.4111, -1.0391],\n",
      "         [ 0.7231,  1.3779, -0.7754,  ...,  0.5107, -2.2246,  0.7900],\n",
      "         [ 1.8770, -0.3726,  0.0321,  ...,  0.1875,  0.8022,  0.2302],\n",
      "         ...,\n",
      "         [ 0.8213, -0.1230,  0.4741,  ...,  0.1212,  0.0745, -0.5869],\n",
      "         [ 0.8008, -0.5933, -0.2690,  ...,  0.7334, -0.0825,  0.3245],\n",
      "         [ 2.4883, -0.0365, -0.1560,  ...,  0.3997,  0.3770, -0.1602]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2595, -0.0715, -0.6475],\n",
      "        [ 0.2566, -0.0615, -0.6367]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives 1.208984375\n",
      "loss = loss_fct(tensor([[ 0.2585, -0.0705, -0.6479],\n",
      "        [ 0.2563, -0.0613, -0.6362]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives 0.9208984375\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2527, -0.0551, -0.6323],\n",
      "        [ 0.2588, -0.0671, -0.6421]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3135,  0.3550, -0.6064,  ...,  2.5410,  1.3984, -1.0381],\n",
      "         [ 0.7061,  1.4746, -0.7656,  ...,  0.5757, -2.2441,  0.7451],\n",
      "         [ 1.8730, -0.3933,  0.0459,  ...,  0.1915,  0.8140,  0.2107],\n",
      "         ...,\n",
      "         [ 0.8364, -0.1389,  0.5054,  ...,  0.0928,  0.0542, -0.5898],\n",
      "         [ 0.8037, -0.5752, -0.1989,  ...,  0.6631, -0.0874,  0.3191],\n",
      "         [ 2.4980, -0.0394, -0.1105,  ...,  0.3818,  0.3569, -0.1442]],\n",
      "\n",
      "        [[ 1.3252,  0.3555, -0.6299,  ...,  2.5664,  1.4326, -1.0371],\n",
      "         [ 0.6714,  1.5156, -0.7480,  ...,  0.4949, -2.1973,  0.8008],\n",
      "         [ 1.8066, -0.3796,  0.0369,  ...,  0.1957,  0.8433,  0.1868],\n",
      "         ...,\n",
      "         [ 0.8154, -0.1348,  0.5142,  ...,  0.1206,  0.1067, -0.6416],\n",
      "         [ 0.7861, -0.5444, -0.2256,  ...,  0.7017, -0.0579,  0.3350],\n",
      "         [ 2.4688, -0.0480, -0.1284,  ...,  0.3853,  0.3828, -0.1469]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2527, -0.0541, -0.6323],\n",
      "        [ 0.2583, -0.0659, -0.6421]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3037,  0.3596, -0.5996,  ...,  2.5586,  1.4229, -1.0498],\n",
      "         [ 0.6860,  1.5059, -0.7861,  ...,  0.5835, -2.2500,  0.7695],\n",
      "         [ 1.8457, -0.4087,  0.0427,  ...,  0.1987,  0.8066,  0.2269],\n",
      "         ...,\n",
      "         [ 0.8428, -0.1459,  0.4734,  ...,  0.0827,  0.0586, -0.5854],\n",
      "         [ 0.7812, -0.5762, -0.2180,  ...,  0.6699, -0.0752,  0.3062],\n",
      "         [ 2.4414, -0.0492, -0.1279,  ...,  0.3845,  0.3711, -0.1453]],\n",
      "\n",
      "        [[ 1.3135,  0.3547, -0.6216,  ...,  2.5645,  1.4355, -1.0420],\n",
      "         [ 0.6587,  1.5000, -0.7466,  ...,  0.5083, -2.1992,  0.8203],\n",
      "         [ 1.8066, -0.3999,  0.0421,  ...,  0.1946,  0.8267,  0.1934],\n",
      "         ...,\n",
      "         [ 0.8291, -0.1396,  0.5010,  ...,  0.0942,  0.1050, -0.6519],\n",
      "         [ 0.7808, -0.5410, -0.2460,  ...,  0.6885, -0.0691,  0.3267],\n",
      "         [ 2.4395, -0.0530, -0.1420,  ...,  0.3792,  0.3799, -0.1533]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2527, -0.0551, -0.6323],\n",
      "        [ 0.2588, -0.0671, -0.6421]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives 1.2021484375\n",
      "loss = loss_fct(tensor([[ 0.2527, -0.0541, -0.6323],\n",
      "        [ 0.2583, -0.0659, -0.6421]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives 0.76025390625\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2600, -0.0714, -0.6499],\n",
      "        [ 0.2588, -0.0652, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3271,  0.3813, -0.6123,  ...,  2.5469,  1.4365, -1.0576],\n",
      "         [ 0.6904,  1.4658, -0.6826,  ...,  0.4854, -2.1738,  0.8521],\n",
      "         [ 1.8691, -0.3738,  0.0213,  ...,  0.1959,  0.8105,  0.2104],\n",
      "         ...,\n",
      "         [ 0.8076, -0.1028,  0.5103,  ...,  0.0976,  0.1089, -0.6011],\n",
      "         [ 0.7886, -0.5781, -0.2089,  ...,  0.6992, -0.0642,  0.3499],\n",
      "         [ 2.5156,  0.0052, -0.1038,  ...,  0.3694,  0.3792, -0.1628]],\n",
      "\n",
      "        [[ 1.3301,  0.3606, -0.6143,  ...,  2.5508,  1.4160, -1.0312],\n",
      "         [ 0.6865,  1.4951, -0.7212,  ...,  0.5459, -2.2051,  0.8159],\n",
      "         [ 1.9199, -0.3586,  0.0045,  ...,  0.2263,  0.8140,  0.2163],\n",
      "         ...,\n",
      "         [ 0.7666, -0.1339,  0.5283,  ...,  0.1321,  0.0967, -0.6284],\n",
      "         [ 0.8022, -0.5869, -0.1954,  ...,  0.6885, -0.0708,  0.3220],\n",
      "         [ 2.4082, -0.0558, -0.1523,  ...,  0.3972,  0.3528, -0.1642]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2615, -0.0722, -0.6504],\n",
      "        [ 0.2585, -0.0651, -0.6348]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3242e+00,  3.8574e-01, -6.0498e-01,  ...,  2.5586e+00,\n",
      "           1.4434e+00, -1.0654e+00],\n",
      "         [ 6.8213e-01,  1.4824e+00, -7.1484e-01,  ...,  5.1562e-01,\n",
      "          -2.1953e+00,  8.5986e-01],\n",
      "         [ 1.8652e+00, -3.8257e-01,  2.2064e-02,  ...,  2.0227e-01,\n",
      "           8.0420e-01,  2.1997e-01],\n",
      "         ...,\n",
      "         [ 8.1543e-01, -1.1133e-01,  4.9463e-01,  ...,  8.7341e-02,\n",
      "           1.1792e-01, -5.9863e-01],\n",
      "         [ 7.7148e-01, -5.8203e-01, -2.1826e-01,  ...,  6.9971e-01,\n",
      "          -5.7861e-02,  3.3740e-01],\n",
      "         [ 2.5020e+00, -2.4414e-03, -1.1481e-01,  ...,  3.7476e-01,\n",
      "           3.9453e-01, -1.5857e-01]],\n",
      "\n",
      "        [[ 1.3242e+00,  3.6426e-01, -6.0352e-01,  ...,  2.5566e+00,\n",
      "           1.4297e+00, -1.0371e+00],\n",
      "         [ 6.7676e-01,  1.5107e+00, -7.3535e-01,  ...,  5.4785e-01,\n",
      "          -2.2109e+00,  8.0566e-01],\n",
      "         [ 1.9141e+00, -3.7476e-01,  1.1383e-02,  ...,  2.2437e-01,\n",
      "           8.0225e-01,  2.2229e-01],\n",
      "         ...,\n",
      "         [ 7.8369e-01, -1.2988e-01,  5.1074e-01,  ...,  1.1627e-01,\n",
      "           9.8145e-02, -6.2598e-01],\n",
      "         [ 7.8906e-01, -5.8496e-01, -2.0166e-01,  ...,  6.8750e-01,\n",
      "          -6.1035e-02,  3.1323e-01],\n",
      "         [ 2.3887e+00, -5.9021e-02, -1.5820e-01,  ...,  3.8770e-01,\n",
      "           3.6719e-01, -1.5308e-01]]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2600, -0.0714, -0.6499],\n",
      "        [ 0.2588, -0.0652, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives 1.201171875\n",
      "loss = loss_fct(tensor([[ 0.2615, -0.0722, -0.6504],\n",
      "        [ 0.2585, -0.0651, -0.6348]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives 1.2099609375\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2581, -0.0699, -0.6445],\n",
      "        [ 0.2537, -0.0579, -0.6348]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3008,  0.4058, -0.5972,  ...,  2.5527,  1.4824, -1.0684],\n",
      "         [ 0.6816,  1.5498, -0.7744,  ...,  0.5537, -2.2617,  0.8423],\n",
      "         [ 1.8418, -0.4028,  0.0630,  ...,  0.1646,  0.8052,  0.2002],\n",
      "         ...,\n",
      "         [ 0.8267, -0.1519,  0.5083,  ...,  0.1025,  0.0774, -0.6484],\n",
      "         [ 0.7915, -0.5552, -0.2380,  ...,  0.6748, -0.0684,  0.3296],\n",
      "         [ 2.4336, -0.0752, -0.1516,  ...,  0.3582,  0.3931, -0.1781]],\n",
      "\n",
      "        [[ 1.3018,  0.3589, -0.6040,  ...,  2.5879,  1.4199, -1.0439],\n",
      "         [ 0.7031,  1.5381, -0.7944,  ...,  0.5557, -2.2754,  0.7627],\n",
      "         [ 1.8516, -0.3506, -0.0143,  ...,  0.1959,  0.7954,  0.2175],\n",
      "         ...,\n",
      "         [ 0.8286, -0.1198,  0.5342,  ...,  0.1174,  0.0991, -0.6030],\n",
      "         [ 0.7578, -0.5728, -0.2629,  ...,  0.7153, -0.0400,  0.3113],\n",
      "         [ 2.4238, -0.0575, -0.1219,  ...,  0.3953,  0.3997, -0.1592]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2576, -0.0703, -0.6445],\n",
      "        [ 0.2544, -0.0583, -0.6353]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.2998,  0.4050, -0.5991,  ...,  2.5566,  1.5000, -1.0703],\n",
      "         [ 0.6753,  1.5635, -0.7676,  ...,  0.5312, -2.2480,  0.8311],\n",
      "         [ 1.8154, -0.3936,  0.0553,  ...,  0.1626,  0.8110,  0.1941],\n",
      "         ...,\n",
      "         [ 0.8174, -0.1396,  0.5059,  ...,  0.1063,  0.0854, -0.6411],\n",
      "         [ 0.7793, -0.5518, -0.2319,  ...,  0.6782, -0.0532,  0.3274],\n",
      "         [ 2.4082, -0.0696, -0.1458,  ...,  0.3521,  0.4055, -0.1786]],\n",
      "\n",
      "        [[ 1.3057,  0.3562, -0.6035,  ...,  2.5781,  1.3857, -1.0400],\n",
      "         [ 0.7109,  1.5254, -0.7896,  ...,  0.6016, -2.3145,  0.7822],\n",
      "         [ 1.8848, -0.3511, -0.0092,  ...,  0.2070,  0.7886,  0.2145],\n",
      "         ...,\n",
      "         [ 0.8296, -0.1368,  0.5552,  ...,  0.1172,  0.0845, -0.6055],\n",
      "         [ 0.7773, -0.5811, -0.2598,  ...,  0.7109, -0.0647,  0.3167],\n",
      "         [ 2.4688, -0.0605, -0.1339,  ...,  0.4104,  0.3826, -0.1525]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2581, -0.0699, -0.6445],\n",
      "        [ 0.2537, -0.0579, -0.6348]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives 1.654296875\n",
      "loss = loss_fct(tensor([[ 0.2576, -0.0703, -0.6445],\n",
      "        [ 0.2544, -0.0583, -0.6353]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives 1.654296875\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2585, -0.0668, -0.6401],\n",
      "        [ 0.2563, -0.0646, -0.6387]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3203,  0.3638, -0.6001,  ...,  2.5586,  1.4023, -1.0664],\n",
      "         [ 0.6860,  1.5078, -0.7695,  ...,  0.5771, -2.2500,  0.7915],\n",
      "         [ 1.8643, -0.3384,  0.0130,  ...,  0.1862,  0.7876,  0.2390],\n",
      "         ...,\n",
      "         [ 0.8096, -0.1387,  0.4573,  ...,  0.0853,  0.0527, -0.5488],\n",
      "         [ 0.7686, -0.5786, -0.2389,  ...,  0.6870, -0.0447,  0.3008],\n",
      "         [ 2.4375, -0.0205, -0.1775,  ...,  0.4158,  0.3765, -0.1532]],\n",
      "\n",
      "        [[ 1.3047,  0.3591, -0.6011,  ...,  2.5586,  1.4082, -1.0557],\n",
      "         [ 0.6582,  1.5518, -0.6875,  ...,  0.4795, -2.1953,  0.8452],\n",
      "         [ 1.8809, -0.3860,  0.0507,  ...,  0.1749,  0.8340,  0.1951],\n",
      "         ...,\n",
      "         [ 0.8188, -0.1127,  0.4709,  ...,  0.0880,  0.0776, -0.5986],\n",
      "         [ 0.7871, -0.5879, -0.2102,  ...,  0.6626, -0.0466,  0.3333],\n",
      "         [ 2.4043, -0.1096, -0.1608,  ...,  0.3892,  0.3684, -0.1915]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2585, -0.0665, -0.6401],\n",
      "        [ 0.2561, -0.0632, -0.6387]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3184,  0.3655, -0.6011,  ...,  2.5586,  1.4121, -1.0684],\n",
      "         [ 0.6831,  1.5254, -0.7617,  ...,  0.5659, -2.2422,  0.7812],\n",
      "         [ 1.8594, -0.3372,  0.0150,  ...,  0.1812,  0.7871,  0.2361],\n",
      "         ...,\n",
      "         [ 0.8052, -0.1237,  0.4607,  ...,  0.0828,  0.0596, -0.5459],\n",
      "         [ 0.7690, -0.5771, -0.2361,  ...,  0.6875, -0.0393,  0.3025],\n",
      "         [ 2.4297, -0.0166, -0.1720,  ...,  0.4106,  0.3813, -0.1487]],\n",
      "\n",
      "        [[ 1.3027,  0.3608, -0.5991,  ...,  2.5625,  1.4238, -1.0625],\n",
      "         [ 0.6484,  1.5566, -0.6885,  ...,  0.4585, -2.1836,  0.8428],\n",
      "         [ 1.8730, -0.3926,  0.0505,  ...,  0.1731,  0.8301,  0.1984],\n",
      "         ...,\n",
      "         [ 0.8242, -0.1028,  0.4580,  ...,  0.0776,  0.0859, -0.5996],\n",
      "         [ 0.7827, -0.5820, -0.2194,  ...,  0.6621, -0.0417,  0.3301],\n",
      "         [ 2.3906, -0.1074, -0.1617,  ...,  0.3831,  0.3760, -0.1953]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2585, -0.0668, -0.6401],\n",
      "        [ 0.2563, -0.0646, -0.6387]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives 1.2060546875\n",
      "loss = loss_fct(tensor([[ 0.2585, -0.0665, -0.6401],\n",
      "        [ 0.2561, -0.0632, -0.6387]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives 0.919921875\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2585, -0.0672, -0.6455],\n",
      "        [ 0.2593, -0.0645, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.3096,  0.3789, -0.6216,  ...,  2.5391,  1.4023, -1.0303],\n",
      "         [ 0.7280,  1.3828, -0.7285,  ...,  0.5073, -2.2246,  0.7671],\n",
      "         [ 1.8457, -0.3689,  0.0602,  ...,  0.1796,  0.8213,  0.1798],\n",
      "         ...,\n",
      "         [ 0.8052, -0.1110,  0.5454,  ...,  0.0801,  0.0664, -0.5840],\n",
      "         [ 0.7979, -0.5454, -0.2462,  ...,  0.6948, -0.0583,  0.3647],\n",
      "         [ 2.5410, -0.0795, -0.0858,  ...,  0.3555,  0.3206, -0.1826]],\n",
      "\n",
      "        [[ 1.3076,  0.3774, -0.6069,  ...,  2.5547,  1.4150, -1.0479],\n",
      "         [ 0.6875,  1.5068, -0.7783,  ...,  0.5601, -2.2734,  0.7305],\n",
      "         [ 1.8750, -0.4060,  0.0435,  ...,  0.2053,  0.8291,  0.2019],\n",
      "         ...,\n",
      "         [ 0.8076, -0.1035,  0.4976,  ...,  0.1023,  0.0903, -0.5640],\n",
      "         [ 0.7822, -0.5630, -0.2749,  ...,  0.7119, -0.0635,  0.3496],\n",
      "         [ 2.4336, -0.0672, -0.1815,  ...,  0.3906,  0.3794, -0.1857]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[ 0.2588, -0.0671, -0.6450],\n",
      "        [ 0.2588, -0.0641, -0.6357]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[ 1.2959,  0.3867, -0.6108,  ...,  2.5547,  1.4277, -1.0391],\n",
      "         [ 0.7070,  1.4102, -0.7773,  ...,  0.5166, -2.2285,  0.7646],\n",
      "         [ 1.8281, -0.3928,  0.0562,  ...,  0.1842,  0.8125,  0.1902],\n",
      "         ...,\n",
      "         [ 0.8193, -0.1049,  0.5044,  ...,  0.0613,  0.0781, -0.5859],\n",
      "         [ 0.7646, -0.5469, -0.2656,  ...,  0.6997, -0.0449,  0.3467],\n",
      "         [ 2.4902, -0.0857, -0.1064,  ...,  0.3521,  0.3496, -0.1787]],\n",
      "\n",
      "        [[ 1.3066,  0.3794, -0.6074,  ...,  2.5547,  1.4180, -1.0488],\n",
      "         [ 0.6865,  1.5088, -0.7861,  ...,  0.5464, -2.2734,  0.7339],\n",
      "         [ 1.8809, -0.4165,  0.0461,  ...,  0.2058,  0.8247,  0.2023],\n",
      "         ...,\n",
      "         [ 0.8130, -0.1016,  0.4939,  ...,  0.0950,  0.0969, -0.5693],\n",
      "         [ 0.7827, -0.5591, -0.2776,  ...,  0.7144, -0.0625,  0.3506],\n",
      "         [ 2.4258, -0.0671, -0.1785,  ...,  0.3896,  0.3845, -0.1868]]],\n",
      "       device='cuda:1', dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[ 0.2585, -0.0672, -0.6455],\n",
      "        [ 0.2593, -0.0645, -0.6357]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives 1.65625\n",
      "loss = loss_fct(tensor([[ 0.2588, -0.0671, -0.6450],\n",
      "        [ 0.2588, -0.0641, -0.6357]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives 1.6552734375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='155' max='4480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 155/4480 02:47 < 1:18:57, 0.91 it/s, Epoch 0.69/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 1], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 1], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 2], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([0, 0], device='cuda:1')) gives nan\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "PerceiverFCS forward gives PerceiverModelOutput(logits=tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>), last_hidden_state=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:1',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None, cross_attentions=None)\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([2, 0], device='cuda:0')) gives nan\n",
      "loss = loss_fct(tensor([[nan, nan, nan],\n",
      "        [nan, nan, nan]], device='cuda:1', dtype=torch.float16,\n",
      "       grad_fn=<ViewBackward0>), tensor([1, 2], device='cuda:1')) gives nan\n"
     ]
    }
   ],
   "source": [
    "perceiver_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
